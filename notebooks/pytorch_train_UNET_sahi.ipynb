{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0.dev20250306\n",
      "0.22.0.dev20250306\n"
     ]
    }
   ],
   "source": [
    "# Start with basic imports\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Set environment variable to avoid OpenMP error\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "print(torch.__version__)\n",
    "\n",
    "# Import torchvision components separately\n",
    "import torchvision\n",
    "print(torchvision.__version__)\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# SAHI imports for version 0.11.20\n",
    "from sahi.slicing import slice_image\n",
    "from sahi.prediction import PredictionResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define segmentation dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if not f.startswith('.')])\n",
    "        self.mask_files = sorted([f for f in os.listdir(mask_dir) if not f.startswith('.')])\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        image = self.transform(image)\n",
    "        mask = self.transform(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def double_conv(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.dconv_down1 = double_conv(in_channels, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)\n",
    "\n",
    "        # Modified maxpool with ceil_mode\n",
    "        self.maxpool = nn.MaxPool2d(2, ceil_mode=True)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(64 + 128, 64)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, out_channels, 1)\n",
    "\n",
    "    def crop_tensor(self, target_tensor, tensor_to_crop):\n",
    "        _, _, H, W = tensor_to_crop.size()\n",
    "        return target_tensor[:, :, :H, :W]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.dconv_down4(x)\n",
    "\n",
    "        # Decoder path with cropping\n",
    "        x = self.upsample(x)\n",
    "        conv3 = self.crop_tensor(conv3, x)\n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        x = self.dconv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        conv2 = self.crop_tensor(conv2, x)\n",
    "        x = torch.cat([x, conv2], dim=1)\n",
    "        x = self.dconv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        conv1 = self.crop_tensor(conv1, x)\n",
    "        x = torch.cat([x, conv1], dim=1)\n",
    "        x = self.dconv_up1(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define class for sahi segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAHISegmentationAdapter(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get all image files\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png', '.tif', '.tiff'))])\n",
    "\n",
    "        # Get corresponding mask files\n",
    "        self.mask_files = []\n",
    "        for img_file in self.image_files:\n",
    "            # Adjust this logic based on how your mask filenames correspond to image filenames\n",
    "            mask_base = os.path.splitext(img_file)[0]\n",
    "            mask_candidates = [\n",
    "                f\"{mask_base}.png\",\n",
    "                f\"{mask_base}.jpg\",\n",
    "                f\"{mask_base}.tif\",\n",
    "                f\"{mask_base}_mask.png\",\n",
    "                f\"{mask_base}_mask.jpg\",\n",
    "                # Add other potential mask filename patterns\n",
    "            ]\n",
    "\n",
    "            found = False\n",
    "            for mask_file in mask_candidates:\n",
    "                if os.path.exists(os.path.join(mask_dir, mask_file)):\n",
    "                    self.mask_files.append(mask_file)\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "            if not found:\n",
    "                print(f\"Warning: No mask found for image {img_file}\")\n",
    "                # You can either skip this image or use a blank mask\n",
    "                # For now, we'll keep the image and create a warning\n",
    "\n",
    "        # Verify we have the same number of images and masks\n",
    "        assert len(self.image_files) == len(self.mask_files), \"Number of images and masks don't match\"\n",
    "\n",
    "        # Print dataset info\n",
    "        print(f\"Found {len(self.image_files)} image-mask pairs\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load mask\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "        # Convert to tensors\n",
    "        image_tensor = transforms.ToTensor()(image)\n",
    "        mask_tensor = transforms.ToTensor()(mask)\n",
    "\n",
    "        # Check if dimensions match\n",
    "        if image_tensor.shape[1:] != mask_tensor.shape[1:]:\n",
    "            print(f\"Fixing dimension mismatch in item {idx}: Image {image_tensor.shape} vs Mask {mask_tensor.shape}\")\n",
    "\n",
    "            # Resize mask to match image dimensions\n",
    "            mask_tensor = F.interpolate(\n",
    "                mask_tensor.unsqueeze(0),  # Add batch dimension\n",
    "                size=(image_tensor.shape[1], image_tensor.shape[2]),\n",
    "                mode='nearest'\n",
    "            ).squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        # Apply additional transformations if provided\n",
    "        if self.transform:\n",
    "            image_tensor, mask_tensor = self.transform(image_tensor, mask_tensor)\n",
    "\n",
    "        return image_tensor, mask_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to properly convert tensors to numpy arrays for SAHI\n",
    "def prepare_for_sahi(tensor):\n",
    "    \"\"\"\n",
    "    Convert a PyTorch tensor to a numpy array format that SAHI can process\n",
    "\n",
    "    Args:\n",
    "        tensor: PyTorch tensor of shape [C, H, W]\n",
    "\n",
    "    Returns:\n",
    "        numpy array of shape [H, W, C] with values in range 0-255 (uint8)\n",
    "    \"\"\"\n",
    "    # Move tensor to CPU if it's on another device\n",
    "    tensor = tensor.cpu()\n",
    "\n",
    "    # Convert from [C, H, W] to [H, W, C]\n",
    "    np_array = tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # If it's a single channel image, remove the channel dimension or repeat to 3 channels\n",
    "    if np_array.shape[2] == 1:\n",
    "        np_array = np.repeat(np_array, 3, axis=2)  # Repeat to 3 channels\n",
    "\n",
    "    # Ensure values are in range 0-255 for uint8\n",
    "    if np_array.dtype == np.float32 or np_array.dtype == np.float64:\n",
    "        if np_array.max() <= 1.0:\n",
    "            np_array = (np_array * 255).astype(np.uint8)\n",
    "        else:\n",
    "            np_array = np_array.astype(np.uint8)\n",
    "\n",
    "    return np_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_with_sahi(model, image, slice_height=512, slice_width=512, overlap_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Process a large image using SAHI slicing technique\n",
    "\n",
    "    Args:\n",
    "        model: Your UNET model wrapped in SAHISegmentationAdapter\n",
    "        image: Input image as numpy array (H, W, C)\n",
    "        slice_height, slice_width: Size of slices\n",
    "        overlap_ratio: Overlap between adjacent slices\n",
    "\n",
    "    Returns:\n",
    "        Full-sized segmentation mask\n",
    "    \"\"\"\n",
    "    # Get image dimensions\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Create empty mask for the full image\n",
    "    full_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Create weight map for blending overlapping regions\n",
    "    weight_map = np.zeros((height, width), dtype=np.float32)\n",
    "\n",
    "    # Slice the image\n",
    "    slices = slice_image(\n",
    "        image=image,\n",
    "        slice_height=slice_height,\n",
    "        slice_width=slice_width,\n",
    "        overlap_height_ratio=overlap_ratio,\n",
    "        overlap_width_ratio=overlap_ratio\n",
    "    )\n",
    "\n",
    "    # Process each slice\n",
    "    for slice_data in slices:\n",
    "        # Get the slice image\n",
    "        slice_image_data = slice_data[\"image\"]\n",
    "\n",
    "        # Get coordinates\n",
    "        x_min, y_min, x_max, y_max = slice_data[\"coordinates\"]\n",
    "\n",
    "        # Predict on this slice\n",
    "        slice_mask = model.predict_single_image(slice_image_data)\n",
    "\n",
    "        # Create weight matrix for this slice (higher in the center, lower at the edges)\n",
    "        h, w = y_max - y_min, x_max - x_min\n",
    "        y, x = np.ogrid[:h, :w]\n",
    "        y_center, x_center = h / 2, w / 2\n",
    "\n",
    "        # Weight falls off with distance from center (gaussian-like)\n",
    "        weight = np.exp(-((x - x_center)**2 / (w/2)**2 + (y - y_center)**2 / (h/2)**2))\n",
    "\n",
    "        # Add weighted mask to full mask\n",
    "        full_mask[y_min:y_max, x_min:x_max] += (slice_mask * weight).astype(np.uint8)\n",
    "        weight_map[y_min:y_max, x_min:x_max] += weight\n",
    "\n",
    "    # Normalize by weight map to blend overlapping regions\n",
    "    # Avoid division by zero\n",
    "    weight_map = np.maximum(weight_map, 1e-10)\n",
    "    full_mask = (full_mask / weight_map).astype(np.uint8)\n",
    "\n",
    "    # Threshold to get binary mask if needed\n",
    "    full_mask = (full_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "    return full_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a collate function with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    masks = [item[1] for item in batch]\n",
    "\n",
    "    # First, ensure masks match their corresponding images in dimensions\n",
    "    aligned_masks = []\n",
    "    for i, (img, msk) in enumerate(zip(images, masks)):\n",
    "        # Check if dimensions don't match\n",
    "        if img.shape[1:] != msk.shape[1:]:\n",
    "            print(f\"Item {i}: Fixing dimension mismatch: Image {img.shape} vs Mask {msk.shape}\")\n",
    "            # Resize mask to match image dimensions\n",
    "            c, h, w = msk.shape\n",
    "            img_h, img_w = img.shape[1:]\n",
    "\n",
    "            # Use interpolate to resize\n",
    "            resized_mask = F.interpolate(\n",
    "                msk.unsqueeze(0),  # Add batch dimension\n",
    "                size=(img_h, img_w),\n",
    "                mode='nearest'\n",
    "            ).squeeze(0)  # Remove batch dimension\n",
    "\n",
    "            aligned_masks.append(resized_mask)\n",
    "        else:\n",
    "            aligned_masks.append(msk)\n",
    "\n",
    "    # Now use the aligned masks\n",
    "    masks = aligned_masks\n",
    "\n",
    "    # Print all dimensions for debugging\n",
    "    for i, (img, msk) in enumerate(zip(images, masks)):\n",
    "        print(f\"Item {i} after alignment: Image {img.shape}, Mask {msk.shape}\")\n",
    "\n",
    "    # Find max dimensions in the batch\n",
    "    # Get the current batch's dimensions\n",
    "    batch_heights = [img.shape[1] for img in images]\n",
    "    batch_widths = [img.shape[2] for img in images]\n",
    "\n",
    "    max_h = max(batch_heights)\n",
    "    max_w = max(batch_widths)\n",
    "\n",
    "    print(f\"Max dimensions in batch: Height={max_h}, Width={max_w}\")\n",
    "\n",
    "    # Set target dimensions to be divisible by 32 (common for U-Net architectures)\n",
    "    target_height = ((max_h + 31) // 32) * 32\n",
    "    target_width = ((max_w + 31) // 32) * 32\n",
    "\n",
    "    print(f\"Target dimensions: Height={target_height}, Width={target_width}\")\n",
    "\n",
    "    # Check if any image is larger than the target\n",
    "    for i, img in enumerate(images):\n",
    "        if img.shape[1] > target_height or img.shape[2] > target_width:\n",
    "            print(f\"Warning: Image {i} with shape {img.shape} is larger than target ({target_height}, {target_width})\")\n",
    "\n",
    "    def process_to_target_size(tensor, target_height, target_width, mode='constant'):\n",
    "        \"\"\"Resize or pad tensor to target size\"\"\"\n",
    "        # Handle both 2D and 3D tensors\n",
    "        if tensor.dim() == 2:\n",
    "            # For 2D tensor (H, W), add channel dimension\n",
    "            tensor = tensor.unsqueeze(0)  # Convert to (1, H, W)\n",
    "\n",
    "        # Now tensor should be 3D (C, H, W)\n",
    "        c, h, w = tensor.shape\n",
    "\n",
    "        # If image is larger than target in any dimension, resize it down\n",
    "        if h > target_height or w > target_width:\n",
    "            print(f\"Resizing tensor from {tensor.shape} to fit within ({target_height}, {target_width})\")\n",
    "            # Resize down maintaining aspect ratio\n",
    "            scale = min(target_height / h, target_width / w)\n",
    "            new_h, new_w = int(h * scale), int(w * scale)\n",
    "            resized = F.interpolate(tensor.unsqueeze(0), size=(new_h, new_w), \n",
    "                                   mode='bilinear' if c == 3 else 'nearest')\n",
    "            tensor = resized.squeeze(0)\n",
    "            c, h, w = tensor.shape\n",
    "\n",
    "        # Now pad to exact target size\n",
    "        pad_h = target_height - h\n",
    "        pad_w = target_width - w\n",
    "\n",
    "        if pad_h < 0:\n",
    "            print(f\"Error: Negative height padding {pad_h} for tensor of shape {tensor.shape}\")\n",
    "            # Force resize instead of padding\n",
    "            tensor = F.interpolate(tensor.unsqueeze(0), size=(target_height, w), \n",
    "                                  mode='bilinear' if c == 3 else 'nearest').squeeze(0)\n",
    "            pad_h = 0\n",
    "\n",
    "        if pad_w < 0:\n",
    "            print(f\"Error: Negative width padding {pad_w} for tensor of shape {tensor.shape}\")\n",
    "            # Force resize instead of padding\n",
    "            tensor = F.interpolate(tensor.unsqueeze(0), size=(h, target_width), \n",
    "                                  mode='bilinear' if c == 3 else 'nearest').squeeze(0)\n",
    "            pad_w = 0\n",
    "\n",
    "        print(f\"Padding tensor from {tensor.shape} with padding (0, {pad_w}, 0, {pad_h})\")\n",
    "        padded_tensor = F.pad(tensor, (0, pad_w, 0, pad_h), mode=mode)\n",
    "        return padded_tensor\n",
    "\n",
    "    # Process images and masks\n",
    "    processed_images = []\n",
    "    processed_masks = []\n",
    "\n",
    "    for i, (img, msk) in enumerate(zip(images, masks)):\n",
    "        print(f\"Processing item {i} - Image: {img.shape}, Mask: {msk.shape}\")\n",
    "\n",
    "        try:\n",
    "            processed_img = process_to_target_size(img, target_height, target_width, mode='reflect')\n",
    "            processed_images.append(processed_img)\n",
    "\n",
    "            # Handle mask based on its dimensionality\n",
    "            processed_mask = process_to_target_size(msk, target_height, target_width, mode='constant')\n",
    "            processed_masks.append(processed_mask)\n",
    "\n",
    "            print(f\"Successfully processed item {i} - Image: {processed_img.shape}, Mask: {processed_mask.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item {i}: {e}\")\n",
    "            # Skip this item if there's an error\n",
    "            continue\n",
    "\n",
    "    # If we had to skip items, make sure we still have something to return\n",
    "    if len(processed_images) == 0:\n",
    "        raise RuntimeError(\"All items in batch were skipped due to processing errors\")\n",
    "\n",
    "    # Stack into batches\n",
    "    batched_images = torch.stack(processed_images)\n",
    "    batched_masks = torch.stack(processed_masks)\n",
    "\n",
    "    print(f\"Final batched shapes - Images: {batched_images.shape}, Masks: {batched_masks.shape}\")\n",
    "    return batched_images, batched_masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 reduction update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upload train/val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datasets\n",
    "train_dataset = SegmentationDataset(\n",
    "    image_dir=\"/Users/aja294/Documents/Hemp_local/projects/pytorch_train_leaf_morpho/data/train/images\",\n",
    "    mask_dir=\"/Users/aja294/Documents/Hemp_local/projects/pytorch_train_leaf_morpho/data/train/masks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SegmentationDataset(\n",
    "    image_dir=\"/Users/aja294/Documents/Hemp_local/projects/pytorch_train_leaf_morpho/data/val/images\",\n",
    "    mask_dir=\"/Users/aja294/Documents/Hemp_local/projects/pytorch_train_leaf_morpho/data/val/masks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = UNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Metal/Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = UNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Updated model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 1\n",
      "Processing - Image shape: torch.Size([3, 2197, 2573]), Mask shape: torch.Size([1, 2197, 2573])\n",
      "Processing - Image shape: torch.Size([3, 2443, 2581]), Mask shape: torch.Size([1, 2443, 2581])\n",
      "Processing - Image shape: torch.Size([3, 1963, 2016]), Mask shape: torch.Size([1, 1963, 2016])\n",
      "Processing - Image shape: torch.Size([3, 2165, 2458]), Mask shape: torch.Size([1, 2165, 2458])\n",
      "Batched shapes - Images: torch.Size([4, 3, 2464, 2592]), Masks: torch.Size([4, 1, 2464, 2592])\n",
      "Processing training batch 1/5\n",
      "Converted image shape: (2464, 2592, 3), mask shape: (2464, 2592, 3)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'coordinates'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[108]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m slice_data \u001b[38;5;129;01min\u001b[39;00m image_slices:\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# Get the slice image and coordinates\u001b[39;00m\n\u001b[32m     42\u001b[39m     slice_image_data = slice_data[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     x_min, y_min, x_max, y_max = \u001b[43mslice_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcoordinates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# Extract corresponding mask slice\u001b[39;00m\n\u001b[32m     46\u001b[39m     slice_mask_data = mask_np[y_min:y_max, x_min:x_max, :]\n",
      "\u001b[31mKeyError\u001b[39m: 'coordinates'"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "slice_height = 512\n",
    "slice_width = 512\n",
    "overlap_ratio = 0.2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting training epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "        print(f\"Processing training batch {batch_idx+1}/{len(train_loader)}\")\n",
    "        batch_loss = 0\n",
    "\n",
    "        # Process each image in the batch\n",
    "        for i in range(images.shape[0]):\n",
    "            # Get single image and mask\n",
    "            image = images[i]\n",
    "            mask = masks[i]\n",
    "\n",
    "            # Convert to SAHI-compatible format\n",
    "            image_np = prepare_for_sahi(image)\n",
    "            mask_np = prepare_for_sahi(mask.unsqueeze(0) if mask.dim() == 2 else mask)\n",
    "\n",
    "            print(f\"Converted image shape: {image_np.shape}, mask shape: {mask_np.shape}\")\n",
    "\n",
    "            # Slice the image\n",
    "            image_slices = slice_image(\n",
    "                image=image_np,\n",
    "                slice_height=slice_height,\n",
    "                slice_width=slice_width,\n",
    "                overlap_height_ratio=overlap_ratio,\n",
    "                overlap_width_ratio=overlap_ratio\n",
    "            )\n",
    "\n",
    "            # Process each slice\n",
    "            slice_losses = []\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for slice_data in image_slices:\n",
    "                # Get the slice image and coordinates\n",
    "                slice_image_data = slice_data[\"image\"]\n",
    "                x_min, y_min, x_max, y_max = slice_data[\"coordinates\"]\n",
    "\n",
    "                # Extract corresponding mask slice\n",
    "                slice_mask_data = mask_np[y_min:y_max, x_min:x_max, :]\n",
    "\n",
    "                # Convert back to tensors\n",
    "                slice_image_tensor = torch.from_numpy(\n",
    "                    slice_image_data.transpose(2, 0, 1)\n",
    "                ).float().div(255.0).unsqueeze(0).to(device)\n",
    "\n",
    "                slice_mask_tensor = torch.from_numpy(\n",
    "                    slice_mask_data[:, :, 0:1].transpose(2, 0, 1)\n",
    "                ).float().div(255.0).unsqueeze(0).to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                slice_output = model(slice_image_tensor)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = F.binary_cross_entropy_with_logits(\n",
    "                    slice_output, \n",
    "                    slice_mask_tensor,\n",
    "                    reduction='mean'\n",
    "                )\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                slice_losses.append(loss.item())\n",
    "\n",
    "            # Update weights after processing all slices\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate average loss for this image\n",
    "            if slice_losses:\n",
    "                image_loss = sum(slice_losses) / len(slice_losses)\n",
    "                batch_loss += image_loss\n",
    "                print(f\"Image {i+1} loss: {image_loss:.4f}\")\n",
    "\n",
    "        # Average loss for the batch\n",
    "        batch_loss /= images.shape[0]\n",
    "        train_loss += batch_loss\n",
    "        print(f\"Batch {batch_idx+1} average loss: {batch_loss:.4f}\")\n",
    "\n",
    "    # Calculate epoch training loss\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    print(f\"Starting validation epoch {epoch+1}\")\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(val_loader):\n",
    "            print(f\"Processing validation batch {batch_idx+1}/{len(val_loader)}\")\n",
    "            batch_loss = 0\n",
    "\n",
    "            # Process each image in the batch\n",
    "            for i in range(images.shape[0]):\n",
    "                # Get single image and mask\n",
    "                image = images[i]\n",
    "                mask = masks[i]\n",
    "\n",
    "                # Convert to SAHI-compatible format\n",
    "                image_np = prepare_for_sahi(image)\n",
    "                mask_np = prepare_for_sahi(mask.unsqueeze(0) if mask.dim() == 2 else mask)\n",
    "\n",
    "                # Slice the image\n",
    "                image_slices = slice_image(\n",
    "                    image=image_np,\n",
    "                    slice_height=slice_height,\n",
    "                    slice_width=slice_width,\n",
    "                    overlap_height_ratio=overlap_ratio,\n",
    "                    overlap_width_ratio=overlap_ratio\n",
    "                )\n",
    "\n",
    "                # Process each slice\n",
    "                slice_losses = []\n",
    "\n",
    "                for slice_data in image_slices:\n",
    "                    # Get the slice image and coordinates\n",
    "                    slice_image_data = slice_data[\"image\"]\n",
    "                    x_min, y_min, x_max, y_max = slice_data[\"coordinates\"]\n",
    "\n",
    "                    # Extract corresponding mask slice\n",
    "                    slice_mask_data = mask_np[y_min:y_max, x_min:x_max, :]\n",
    "\n",
    "                    # Convert back to tensors\n",
    "                    slice_image_tensor = torch.from_numpy(\n",
    "                        slice_image_data.transpose(2, 0, 1)\n",
    "                    ).float().div(255.0).unsqueeze(0).to(device)\n",
    "\n",
    "                    slice_mask_tensor = torch.from_numpy(\n",
    "                        slice_mask_data[:, :, 0:1].transpose(2, 0, 1)\n",
    "                    ).float().div(255.0).unsqueeze(0).to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    slice_output = model(slice_image_tensor)\n",
    "\n",
    "                    # Calculate loss\n",
    "                    loss = F.binary_cross_entropy_with_logits(\n",
    "                        slice_output, \n",
    "                        slice_mask_tensor,\n",
    "                        reduction='mean'\n",
    "                    )\n",
    "\n",
    "                    slice_losses.append(loss.item())\n",
    "\n",
    "                # Calculate average loss for this image\n",
    "                if slice_losses:\n",
    "                    image_loss = sum(slice_losses) / len(slice_losses)\n",
    "                    batch_loss += image_loss\n",
    "\n",
    "            # Average loss for the batch\n",
    "            batch_loss /= images.shape[0]\n",
    "            val_loss += batch_loss\n",
    "            print(f\"Validation batch {batch_idx+1} average loss: {batch_loss:.4f}\")\n",
    "\n",
    "        # Calculate epoch validation loss\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "    }, f'model_checkpoint_epoch_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_slice_image(image_np, slice_height, slice_width, overlap_ratio):\n",
    "    \"\"\"\n",
    "    Custom function to slice a numpy image array into smaller patches with overlap\n",
    "\n",
    "    Args:\n",
    "        image_np: Numpy array of shape [H, W, C]\n",
    "        slice_height, slice_width: Size of slices\n",
    "        overlap_ratio: Overlap between slices (0-1)\n",
    "\n",
    "    Process:\n",
    "        Groups images into batches of 4\n",
    "        Aligns dimensions within each batch by padding to a common size\n",
    "        Prepares each batch for SAHI slicing\n",
    "        Processes 5 batches in total per epoch (based on \"Processing training batch 3/5\")\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries with 'image' and 'coordinates' keys\n",
    "    \"\"\"\n",
    "    height, width = image_np.shape[:2]\n",
    "    stride_h = int(slice_height * (1 - overlap_ratio))\n",
    "    stride_w = int(slice_width * (1 - overlap_ratio))\n",
    "\n",
    "    slices = []\n",
    "    for y in range(0, height, stride_h):\n",
    "        for x in range(0, width, stride_w):\n",
    "            # Calculate slice coordinates\n",
    "            x_min = x\n",
    "            y_min = y\n",
    "            x_max = min(x + slice_width, width)\n",
    "            y_max = min(y + slice_height, height)\n",
    "\n",
    "            # Handle edge cases - ensure slices are of size (slice_height, slice_width) when possible\n",
    "            if x_max - x_min < slice_width and x_min > 0:\n",
    "                x_min = max(0, x_max - slice_width)\n",
    "            if y_max - y_min < slice_height and y_min > 0:\n",
    "                y_min = max(0, y_max - slice_height)\n",
    "\n",
    "            # Extract slice\n",
    "            slice_image = image_np[y_min:y_max, x_min:x_max, :]\n",
    "\n",
    "            # Create slice data in the expected format\n",
    "            slice_data = {\n",
    "                \"image\": slice_image,\n",
    "                \"coordinates\": (x_min, y_min, x_max, y_max)\n",
    "            }\n",
    "\n",
    "            slices.append(slice_data)\n",
    "\n",
    "    return slices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated training loop THIS ONE WORKS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "Starting training epoch 1\n",
      "Item 0 after alignment: Image torch.Size([3, 2199, 2639]), Mask torch.Size([1, 2199, 2639])\n",
      "Item 1 after alignment: Image torch.Size([3, 2197, 2573]), Mask torch.Size([1, 2197, 2573])\n",
      "Item 2 after alignment: Image torch.Size([3, 2525, 2908]), Mask torch.Size([1, 2525, 2908])\n",
      "Item 3 after alignment: Image torch.Size([3, 2443, 2581]), Mask torch.Size([1, 2443, 2581])\n",
      "Max dimensions in batch: Height=2525, Width=2908\n",
      "Target dimensions: Height=2528, Width=2912\n",
      "Processing item 0 - Image: torch.Size([3, 2199, 2639]), Mask: torch.Size([1, 2199, 2639])\n",
      "Padding tensor from torch.Size([3, 2199, 2639]) with padding (0, 273, 0, 329)\n",
      "Padding tensor from torch.Size([1, 2199, 2639]) with padding (0, 273, 0, 329)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 1 - Image: torch.Size([3, 2197, 2573]), Mask: torch.Size([1, 2197, 2573])\n",
      "Padding tensor from torch.Size([3, 2197, 2573]) with padding (0, 339, 0, 331)\n",
      "Padding tensor from torch.Size([1, 2197, 2573]) with padding (0, 339, 0, 331)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 2 - Image: torch.Size([3, 2525, 2908]), Mask: torch.Size([1, 2525, 2908])\n",
      "Padding tensor from torch.Size([3, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Padding tensor from torch.Size([1, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 3 - Image: torch.Size([3, 2443, 2581]), Mask: torch.Size([1, 2443, 2581])\n",
      "Padding tensor from torch.Size([3, 2443, 2581]) with padding (0, 331, 0, 85)\n",
      "Padding tensor from torch.Size([1, 2443, 2581]) with padding (0, 331, 0, 85)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2528, 2912]), Masks: torch.Size([4, 1, 2528, 2912])\n",
      "Processing training batch 1/5\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 1 loss: 0.8124\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 2 loss: 0.7828\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 3 loss: 0.7416\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 4 loss: 0.7126\n",
      "Batch 1 average loss: 0.7623\n",
      "Item 0 after alignment: Image torch.Size([3, 2593, 2699]), Mask torch.Size([1, 2593, 2699])\n",
      "Item 1 after alignment: Image torch.Size([3, 2197, 2302]), Mask torch.Size([1, 2197, 2302])\n",
      "Item 2 after alignment: Image torch.Size([3, 2205, 2468]), Mask torch.Size([1, 2205, 2468])\n",
      "Item 3 after alignment: Image torch.Size([3, 2454, 2649]), Mask torch.Size([1, 2454, 2649])\n",
      "Max dimensions in batch: Height=2593, Width=2699\n",
      "Target dimensions: Height=2624, Width=2720\n",
      "Processing item 0 - Image: torch.Size([3, 2593, 2699]), Mask: torch.Size([1, 2593, 2699])\n",
      "Padding tensor from torch.Size([3, 2593, 2699]) with padding (0, 21, 0, 31)\n",
      "Padding tensor from torch.Size([1, 2593, 2699]) with padding (0, 21, 0, 31)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 1 - Image: torch.Size([3, 2197, 2302]), Mask: torch.Size([1, 2197, 2302])\n",
      "Padding tensor from torch.Size([3, 2197, 2302]) with padding (0, 418, 0, 427)\n",
      "Padding tensor from torch.Size([1, 2197, 2302]) with padding (0, 418, 0, 427)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 2 - Image: torch.Size([3, 2205, 2468]), Mask: torch.Size([1, 2205, 2468])\n",
      "Padding tensor from torch.Size([3, 2205, 2468]) with padding (0, 252, 0, 419)\n",
      "Padding tensor from torch.Size([1, 2205, 2468]) with padding (0, 252, 0, 419)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 3 - Image: torch.Size([3, 2454, 2649]), Mask: torch.Size([1, 2454, 2649])\n",
      "Padding tensor from torch.Size([3, 2454, 2649]) with padding (0, 71, 0, 170)\n",
      "Padding tensor from torch.Size([1, 2454, 2649]) with padding (0, 71, 0, 170)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2624, 2720]), Masks: torch.Size([4, 1, 2624, 2720])\n",
      "Processing training batch 2/5\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 1 loss: 0.6877\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 2 loss: 0.6856\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 3 loss: 0.6624\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 4 loss: 0.6431\n",
      "Batch 2 average loss: 0.6697\n",
      "Item 0 after alignment: Image torch.Size([3, 1679, 1972]), Mask torch.Size([1, 1679, 1972])\n",
      "Item 1 after alignment: Image torch.Size([3, 2146, 2256]), Mask torch.Size([1, 2146, 2256])\n",
      "Item 2 after alignment: Image torch.Size([3, 1870, 2232]), Mask torch.Size([1, 1870, 2232])\n",
      "Item 3 after alignment: Image torch.Size([3, 2124, 2633]), Mask torch.Size([1, 2124, 2633])\n",
      "Max dimensions in batch: Height=2146, Width=2633\n",
      "Target dimensions: Height=2176, Width=2656\n",
      "Processing item 0 - Image: torch.Size([3, 1679, 1972]), Mask: torch.Size([1, 1679, 1972])\n",
      "Padding tensor from torch.Size([3, 1679, 1972]) with padding (0, 684, 0, 497)\n",
      "Padding tensor from torch.Size([1, 1679, 1972]) with padding (0, 684, 0, 497)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2176, 2656]), Mask: torch.Size([1, 2176, 2656])\n",
      "Processing item 1 - Image: torch.Size([3, 2146, 2256]), Mask: torch.Size([1, 2146, 2256])\n",
      "Padding tensor from torch.Size([3, 2146, 2256]) with padding (0, 400, 0, 30)\n",
      "Padding tensor from torch.Size([1, 2146, 2256]) with padding (0, 400, 0, 30)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2176, 2656]), Mask: torch.Size([1, 2176, 2656])\n",
      "Processing item 2 - Image: torch.Size([3, 1870, 2232]), Mask: torch.Size([1, 1870, 2232])\n",
      "Padding tensor from torch.Size([3, 1870, 2232]) with padding (0, 424, 0, 306)\n",
      "Padding tensor from torch.Size([1, 1870, 2232]) with padding (0, 424, 0, 306)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2176, 2656]), Mask: torch.Size([1, 2176, 2656])\n",
      "Processing item 3 - Image: torch.Size([3, 2124, 2633]), Mask: torch.Size([1, 2124, 2633])\n",
      "Padding tensor from torch.Size([3, 2124, 2633]) with padding (0, 23, 0, 52)\n",
      "Padding tensor from torch.Size([1, 2124, 2633]) with padding (0, 23, 0, 52)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2176, 2656]), Mask: torch.Size([1, 2176, 2656])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2176, 2656]), Masks: torch.Size([4, 1, 2176, 2656])\n",
      "Processing training batch 3/5\n",
      "Converted image shape: (2176, 2656, 3), mask shape: (2176, 2656, 1)\n",
      "Image 1 loss: 0.6491\n",
      "Converted image shape: (2176, 2656, 3), mask shape: (2176, 2656, 1)\n",
      "Image 2 loss: 0.6526\n",
      "Converted image shape: (2176, 2656, 3), mask shape: (2176, 2656, 1)\n",
      "Image 3 loss: 0.6423\n",
      "Converted image shape: (2176, 2656, 3), mask shape: (2176, 2656, 1)\n",
      "Image 4 loss: 0.6142\n",
      "Batch 3 average loss: 0.6395\n",
      "Item 0 after alignment: Image torch.Size([3, 1963, 2016]), Mask torch.Size([1, 1963, 2016])\n",
      "Item 1 after alignment: Image torch.Size([3, 1743, 2050]), Mask torch.Size([1, 1743, 2050])\n",
      "Item 2 after alignment: Image torch.Size([3, 2042, 2337]), Mask torch.Size([1, 2042, 2337])\n",
      "Item 3 after alignment: Image torch.Size([3, 2350, 2472]), Mask torch.Size([1, 2350, 2472])\n",
      "Max dimensions in batch: Height=2350, Width=2472\n",
      "Target dimensions: Height=2368, Width=2496\n",
      "Processing item 0 - Image: torch.Size([3, 1963, 2016]), Mask: torch.Size([1, 1963, 2016])\n",
      "Padding tensor from torch.Size([3, 1963, 2016]) with padding (0, 480, 0, 405)\n",
      "Padding tensor from torch.Size([1, 1963, 2016]) with padding (0, 480, 0, 405)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2368, 2496]), Mask: torch.Size([1, 2368, 2496])\n",
      "Processing item 1 - Image: torch.Size([3, 1743, 2050]), Mask: torch.Size([1, 1743, 2050])\n",
      "Padding tensor from torch.Size([3, 1743, 2050]) with padding (0, 446, 0, 625)\n",
      "Padding tensor from torch.Size([1, 1743, 2050]) with padding (0, 446, 0, 625)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2368, 2496]), Mask: torch.Size([1, 2368, 2496])\n",
      "Processing item 2 - Image: torch.Size([3, 2042, 2337]), Mask: torch.Size([1, 2042, 2337])\n",
      "Padding tensor from torch.Size([3, 2042, 2337]) with padding (0, 159, 0, 326)\n",
      "Padding tensor from torch.Size([1, 2042, 2337]) with padding (0, 159, 0, 326)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2368, 2496]), Mask: torch.Size([1, 2368, 2496])\n",
      "Processing item 3 - Image: torch.Size([3, 2350, 2472]), Mask: torch.Size([1, 2350, 2472])\n",
      "Padding tensor from torch.Size([3, 2350, 2472]) with padding (0, 24, 0, 18)\n",
      "Padding tensor from torch.Size([1, 2350, 2472]) with padding (0, 24, 0, 18)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2368, 2496]), Mask: torch.Size([1, 2368, 2496])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2368, 2496]), Masks: torch.Size([4, 1, 2368, 2496])\n",
      "Processing training batch 4/5\n",
      "Converted image shape: (2368, 2496, 3), mask shape: (2368, 2496, 1)\n",
      "Image 1 loss: 0.6154\n",
      "Converted image shape: (2368, 2496, 3), mask shape: (2368, 2496, 1)\n",
      "Image 2 loss: 0.6163\n",
      "Converted image shape: (2368, 2496, 3), mask shape: (2368, 2496, 1)\n",
      "Image 3 loss: 0.6155\n",
      "Converted image shape: (2368, 2496, 3), mask shape: (2368, 2496, 1)\n",
      "Image 4 loss: 0.6016\n",
      "Batch 4 average loss: 0.6122\n",
      "Item 0 after alignment: Image torch.Size([3, 1837, 2049]), Mask torch.Size([1, 1837, 2049])\n",
      "Item 1 after alignment: Image torch.Size([3, 2522, 2727]), Mask torch.Size([1, 2522, 2727])\n",
      "Item 2 after alignment: Image torch.Size([3, 2165, 2458]), Mask torch.Size([1, 2165, 2458])\n",
      "Max dimensions in batch: Height=2522, Width=2727\n",
      "Target dimensions: Height=2528, Width=2752\n",
      "Processing item 0 - Image: torch.Size([3, 1837, 2049]), Mask: torch.Size([1, 1837, 2049])\n",
      "Padding tensor from torch.Size([3, 1837, 2049]) with padding (0, 703, 0, 691)\n",
      "Padding tensor from torch.Size([1, 1837, 2049]) with padding (0, 703, 0, 691)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 1 - Image: torch.Size([3, 2522, 2727]), Mask: torch.Size([1, 2522, 2727])\n",
      "Padding tensor from torch.Size([3, 2522, 2727]) with padding (0, 25, 0, 6)\n",
      "Padding tensor from torch.Size([1, 2522, 2727]) with padding (0, 25, 0, 6)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 2 - Image: torch.Size([3, 2165, 2458]), Mask: torch.Size([1, 2165, 2458])\n",
      "Padding tensor from torch.Size([3, 2165, 2458]) with padding (0, 294, 0, 363)\n",
      "Padding tensor from torch.Size([1, 2165, 2458]) with padding (0, 294, 0, 363)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Final batched shapes - Images: torch.Size([3, 3, 2528, 2752]), Masks: torch.Size([3, 1, 2528, 2752])\n",
      "Processing training batch 5/5\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 1 loss: 0.6603\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 2 loss: 0.5770\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 3 loss: 0.5692\n",
      "Batch 5 average loss: 0.6022\n",
      "Starting validation epoch 1\n",
      "Item 0 after alignment: Image torch.Size([3, 2067, 2332]), Mask torch.Size([1, 2067, 2332])\n",
      "Item 1 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Item 2 after alignment: Image torch.Size([3, 1972, 2400]), Mask torch.Size([1, 1972, 2400])\n",
      "Item 3 after alignment: Image torch.Size([3, 1681, 2144]), Mask torch.Size([1, 1681, 2144])\n",
      "Max dimensions in batch: Height=2067, Width=2400\n",
      "Target dimensions: Height=2080, Width=2400\n",
      "Processing item 0 - Image: torch.Size([3, 2067, 2332]), Mask: torch.Size([1, 2067, 2332])\n",
      "Padding tensor from torch.Size([3, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Padding tensor from torch.Size([1, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 1 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 2 - Image: torch.Size([3, 1972, 2400]), Mask: torch.Size([1, 1972, 2400])\n",
      "Padding tensor from torch.Size([3, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Padding tensor from torch.Size([1, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 3 - Image: torch.Size([3, 1681, 2144]), Mask: torch.Size([1, 1681, 2144])\n",
      "Padding tensor from torch.Size([3, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Padding tensor from torch.Size([1, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2080, 2400]), Masks: torch.Size([4, 1, 2080, 2400])\n",
      "Processing validation batch 1/2\n",
      "Validation image 1 loss: 6.1830\n",
      "Validation image 2 loss: 1.8949\n",
      "Validation image 3 loss: 1.7394\n",
      "Validation image 4 loss: 2.1748\n",
      "Validation batch 1 average loss: 2.9980\n",
      "Item 0 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Max dimensions in batch: Height=1705, Width=1902\n",
      "Target dimensions: Height=1728, Width=1920\n",
      "Processing item 0 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 1728, 1920]), Mask: torch.Size([1, 1728, 1920])\n",
      "Final batched shapes - Images: torch.Size([1, 3, 1728, 1920]), Masks: torch.Size([1, 1, 1728, 1920])\n",
      "Processing validation batch 2/2\n",
      "Validation image 1 loss: 2.0119\n",
      "Validation batch 2 average loss: 2.0119\n",
      "Epoch 1/10, Train Loss: 0.6572, Val Loss: 2.5049\n",
      "Starting training epoch 2\n",
      "Item 0 after alignment: Image torch.Size([3, 2525, 2908]), Mask torch.Size([1, 2525, 2908])\n",
      "Item 1 after alignment: Image torch.Size([3, 1870, 2232]), Mask torch.Size([1, 1870, 2232])\n",
      "Item 2 after alignment: Image torch.Size([3, 1743, 2050]), Mask torch.Size([1, 1743, 2050])\n",
      "Item 3 after alignment: Image torch.Size([3, 2146, 2256]), Mask torch.Size([1, 2146, 2256])\n",
      "Max dimensions in batch: Height=2525, Width=2908\n",
      "Target dimensions: Height=2528, Width=2912\n",
      "Processing item 0 - Image: torch.Size([3, 2525, 2908]), Mask: torch.Size([1, 2525, 2908])\n",
      "Padding tensor from torch.Size([3, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Padding tensor from torch.Size([1, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 1 - Image: torch.Size([3, 1870, 2232]), Mask: torch.Size([1, 1870, 2232])\n",
      "Padding tensor from torch.Size([3, 1870, 2232]) with padding (0, 680, 0, 658)\n",
      "Padding tensor from torch.Size([1, 1870, 2232]) with padding (0, 680, 0, 658)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 2 - Image: torch.Size([3, 1743, 2050]), Mask: torch.Size([1, 1743, 2050])\n",
      "Padding tensor from torch.Size([3, 1743, 2050]) with padding (0, 862, 0, 785)\n",
      "Padding tensor from torch.Size([1, 1743, 2050]) with padding (0, 862, 0, 785)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 3 - Image: torch.Size([3, 2146, 2256]), Mask: torch.Size([1, 2146, 2256])\n",
      "Padding tensor from torch.Size([3, 2146, 2256]) with padding (0, 656, 0, 382)\n",
      "Padding tensor from torch.Size([1, 2146, 2256]) with padding (0, 656, 0, 382)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2528, 2912]), Masks: torch.Size([4, 1, 2528, 2912])\n",
      "Processing training batch 1/5\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 1 loss: 0.5774\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 2 loss: 0.6059\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 3 loss: 0.6039\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 4 loss: 0.5983\n",
      "Batch 1 average loss: 0.5963\n",
      "Item 0 after alignment: Image torch.Size([3, 2350, 2472]), Mask torch.Size([1, 2350, 2472])\n",
      "Item 1 after alignment: Image torch.Size([3, 2454, 2649]), Mask torch.Size([1, 2454, 2649])\n",
      "Item 2 after alignment: Image torch.Size([3, 2522, 2727]), Mask torch.Size([1, 2522, 2727])\n",
      "Item 3 after alignment: Image torch.Size([3, 2197, 2302]), Mask torch.Size([1, 2197, 2302])\n",
      "Max dimensions in batch: Height=2522, Width=2727\n",
      "Target dimensions: Height=2528, Width=2752\n",
      "Processing item 0 - Image: torch.Size([3, 2350, 2472]), Mask: torch.Size([1, 2350, 2472])\n",
      "Padding tensor from torch.Size([3, 2350, 2472]) with padding (0, 280, 0, 178)\n",
      "Padding tensor from torch.Size([1, 2350, 2472]) with padding (0, 280, 0, 178)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 1 - Image: torch.Size([3, 2454, 2649]), Mask: torch.Size([1, 2454, 2649])\n",
      "Padding tensor from torch.Size([3, 2454, 2649]) with padding (0, 103, 0, 74)\n",
      "Padding tensor from torch.Size([1, 2454, 2649]) with padding (0, 103, 0, 74)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 2 - Image: torch.Size([3, 2522, 2727]), Mask: torch.Size([1, 2522, 2727])\n",
      "Padding tensor from torch.Size([3, 2522, 2727]) with padding (0, 25, 0, 6)\n",
      "Padding tensor from torch.Size([1, 2522, 2727]) with padding (0, 25, 0, 6)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 3 - Image: torch.Size([3, 2197, 2302]), Mask: torch.Size([1, 2197, 2302])\n",
      "Padding tensor from torch.Size([3, 2197, 2302]) with padding (0, 450, 0, 331)\n",
      "Padding tensor from torch.Size([1, 2197, 2302]) with padding (0, 450, 0, 331)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2528, 2752]), Masks: torch.Size([4, 1, 2528, 2752])\n",
      "Processing training batch 2/5\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 1 loss: 0.5596\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 2 loss: 0.5443\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 3 loss: 0.5433\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 4 loss: 0.5643\n",
      "Batch 2 average loss: 0.5529\n",
      "Item 0 after alignment: Image torch.Size([3, 2443, 2581]), Mask torch.Size([1, 2443, 2581])\n",
      "Item 1 after alignment: Image torch.Size([3, 2165, 2458]), Mask torch.Size([1, 2165, 2458])\n",
      "Item 2 after alignment: Image torch.Size([3, 2593, 2699]), Mask torch.Size([1, 2593, 2699])\n",
      "Item 3 after alignment: Image torch.Size([3, 1837, 2049]), Mask torch.Size([1, 1837, 2049])\n",
      "Max dimensions in batch: Height=2593, Width=2699\n",
      "Target dimensions: Height=2624, Width=2720\n",
      "Processing item 0 - Image: torch.Size([3, 2443, 2581]), Mask: torch.Size([1, 2443, 2581])\n",
      "Padding tensor from torch.Size([3, 2443, 2581]) with padding (0, 139, 0, 181)\n",
      "Padding tensor from torch.Size([1, 2443, 2581]) with padding (0, 139, 0, 181)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 1 - Image: torch.Size([3, 2165, 2458]), Mask: torch.Size([1, 2165, 2458])\n",
      "Padding tensor from torch.Size([3, 2165, 2458]) with padding (0, 262, 0, 459)\n",
      "Padding tensor from torch.Size([1, 2165, 2458]) with padding (0, 262, 0, 459)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 2 - Image: torch.Size([3, 2593, 2699]), Mask: torch.Size([1, 2593, 2699])\n",
      "Padding tensor from torch.Size([3, 2593, 2699]) with padding (0, 21, 0, 31)\n",
      "Padding tensor from torch.Size([1, 2593, 2699]) with padding (0, 21, 0, 31)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 3 - Image: torch.Size([3, 1837, 2049]), Mask: torch.Size([1, 1837, 2049])\n",
      "Padding tensor from torch.Size([3, 1837, 2049]) with padding (0, 671, 0, 787)\n",
      "Padding tensor from torch.Size([1, 1837, 2049]) with padding (0, 671, 0, 787)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2624, 2720]), Masks: torch.Size([4, 1, 2624, 2720])\n",
      "Processing training batch 3/5\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 1 loss: 0.5351\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 2 loss: 0.5293\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 3 loss: 0.5375\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 4 loss: 0.5992\n",
      "Batch 3 average loss: 0.5503\n",
      "Item 0 after alignment: Image torch.Size([3, 2042, 2337]), Mask torch.Size([1, 2042, 2337])\n",
      "Item 1 after alignment: Image torch.Size([3, 2205, 2468]), Mask torch.Size([1, 2205, 2468])\n",
      "Item 2 after alignment: Image torch.Size([3, 1679, 1972]), Mask torch.Size([1, 1679, 1972])\n",
      "Item 3 after alignment: Image torch.Size([3, 2197, 2573]), Mask torch.Size([1, 2197, 2573])\n",
      "Max dimensions in batch: Height=2205, Width=2573\n",
      "Target dimensions: Height=2208, Width=2592\n",
      "Processing item 0 - Image: torch.Size([3, 2042, 2337]), Mask: torch.Size([1, 2042, 2337])\n",
      "Padding tensor from torch.Size([3, 2042, 2337]) with padding (0, 255, 0, 166)\n",
      "Padding tensor from torch.Size([1, 2042, 2337]) with padding (0, 255, 0, 166)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2208, 2592]), Mask: torch.Size([1, 2208, 2592])\n",
      "Processing item 1 - Image: torch.Size([3, 2205, 2468]), Mask: torch.Size([1, 2205, 2468])\n",
      "Padding tensor from torch.Size([3, 2205, 2468]) with padding (0, 124, 0, 3)\n",
      "Padding tensor from torch.Size([1, 2205, 2468]) with padding (0, 124, 0, 3)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2208, 2592]), Mask: torch.Size([1, 2208, 2592])\n",
      "Processing item 2 - Image: torch.Size([3, 1679, 1972]), Mask: torch.Size([1, 1679, 1972])\n",
      "Padding tensor from torch.Size([3, 1679, 1972]) with padding (0, 620, 0, 529)\n",
      "Padding tensor from torch.Size([1, 1679, 1972]) with padding (0, 620, 0, 529)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2208, 2592]), Mask: torch.Size([1, 2208, 2592])\n",
      "Processing item 3 - Image: torch.Size([3, 2197, 2573]), Mask: torch.Size([1, 2197, 2573])\n",
      "Padding tensor from torch.Size([3, 2197, 2573]) with padding (0, 19, 0, 11)\n",
      "Padding tensor from torch.Size([1, 2197, 2573]) with padding (0, 19, 0, 11)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2208, 2592]), Mask: torch.Size([1, 2208, 2592])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2208, 2592]), Masks: torch.Size([4, 1, 2208, 2592])\n",
      "Processing training batch 4/5\n",
      "Converted image shape: (2208, 2592, 3), mask shape: (2208, 2592, 1)\n",
      "Image 1 loss: 0.5463\n",
      "Converted image shape: (2208, 2592, 3), mask shape: (2208, 2592, 1)\n",
      "Image 2 loss: 0.5298\n",
      "Converted image shape: (2208, 2592, 3), mask shape: (2208, 2592, 1)\n",
      "Image 3 loss: 0.5483\n",
      "Converted image shape: (2208, 2592, 3), mask shape: (2208, 2592, 1)\n",
      "Image 4 loss: 0.5174\n",
      "Batch 4 average loss: 0.5355\n",
      "Item 0 after alignment: Image torch.Size([3, 2124, 2633]), Mask torch.Size([1, 2124, 2633])\n",
      "Item 1 after alignment: Image torch.Size([3, 2199, 2639]), Mask torch.Size([1, 2199, 2639])\n",
      "Item 2 after alignment: Image torch.Size([3, 1963, 2016]), Mask torch.Size([1, 1963, 2016])\n",
      "Max dimensions in batch: Height=2199, Width=2639\n",
      "Target dimensions: Height=2208, Width=2656\n",
      "Processing item 0 - Image: torch.Size([3, 2124, 2633]), Mask: torch.Size([1, 2124, 2633])\n",
      "Padding tensor from torch.Size([3, 2124, 2633]) with padding (0, 23, 0, 84)\n",
      "Padding tensor from torch.Size([1, 2124, 2633]) with padding (0, 23, 0, 84)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Processing item 1 - Image: torch.Size([3, 2199, 2639]), Mask: torch.Size([1, 2199, 2639])\n",
      "Padding tensor from torch.Size([3, 2199, 2639]) with padding (0, 17, 0, 9)\n",
      "Padding tensor from torch.Size([1, 2199, 2639]) with padding (0, 17, 0, 9)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Processing item 2 - Image: torch.Size([3, 1963, 2016]), Mask: torch.Size([1, 1963, 2016])\n",
      "Padding tensor from torch.Size([3, 1963, 2016]) with padding (0, 640, 0, 245)\n",
      "Padding tensor from torch.Size([1, 1963, 2016]) with padding (0, 640, 0, 245)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Final batched shapes - Images: torch.Size([3, 3, 2208, 2656]), Masks: torch.Size([3, 1, 2208, 2656])\n",
      "Processing training batch 5/5\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 1 loss: 0.5097\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 2 loss: 0.5376\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 3 loss: 0.5115\n",
      "Batch 5 average loss: 0.5196\n",
      "Starting validation epoch 2\n",
      "Item 0 after alignment: Image torch.Size([3, 2067, 2332]), Mask torch.Size([1, 2067, 2332])\n",
      "Item 1 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Item 2 after alignment: Image torch.Size([3, 1972, 2400]), Mask torch.Size([1, 1972, 2400])\n",
      "Item 3 after alignment: Image torch.Size([3, 1681, 2144]), Mask torch.Size([1, 1681, 2144])\n",
      "Max dimensions in batch: Height=2067, Width=2400\n",
      "Target dimensions: Height=2080, Width=2400\n",
      "Processing item 0 - Image: torch.Size([3, 2067, 2332]), Mask: torch.Size([1, 2067, 2332])\n",
      "Padding tensor from torch.Size([3, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Padding tensor from torch.Size([1, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 1 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 2 - Image: torch.Size([3, 1972, 2400]), Mask: torch.Size([1, 1972, 2400])\n",
      "Padding tensor from torch.Size([3, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Padding tensor from torch.Size([1, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 3 - Image: torch.Size([3, 1681, 2144]), Mask: torch.Size([1, 1681, 2144])\n",
      "Padding tensor from torch.Size([3, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Padding tensor from torch.Size([1, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2080, 2400]), Masks: torch.Size([4, 1, 2080, 2400])\n",
      "Processing validation batch 1/2\n",
      "Validation image 1 loss: 10.1731\n",
      "Validation image 2 loss: 2.5067\n",
      "Validation image 3 loss: 1.6789\n",
      "Validation image 4 loss: 2.6816\n",
      "Validation batch 1 average loss: 4.2601\n",
      "Item 0 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Max dimensions in batch: Height=1705, Width=1902\n",
      "Target dimensions: Height=1728, Width=1920\n",
      "Processing item 0 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 1728, 1920]), Mask: torch.Size([1, 1728, 1920])\n",
      "Final batched shapes - Images: torch.Size([1, 3, 1728, 1920]), Masks: torch.Size([1, 1, 1728, 1920])\n",
      "Processing validation batch 2/2\n",
      "Validation image 1 loss: 2.9400\n",
      "Validation batch 2 average loss: 2.9400\n",
      "Epoch 2/10, Train Loss: 0.5509, Val Loss: 3.6000\n",
      "Starting training epoch 3\n",
      "Item 0 after alignment: Image torch.Size([3, 2593, 2699]), Mask torch.Size([1, 2593, 2699])\n",
      "Item 1 after alignment: Image torch.Size([3, 2197, 2573]), Mask torch.Size([1, 2197, 2573])\n",
      "Item 2 after alignment: Image torch.Size([3, 2454, 2649]), Mask torch.Size([1, 2454, 2649])\n",
      "Item 3 after alignment: Image torch.Size([3, 1743, 2050]), Mask torch.Size([1, 1743, 2050])\n",
      "Max dimensions in batch: Height=2593, Width=2699\n",
      "Target dimensions: Height=2624, Width=2720\n",
      "Processing item 0 - Image: torch.Size([3, 2593, 2699]), Mask: torch.Size([1, 2593, 2699])\n",
      "Padding tensor from torch.Size([3, 2593, 2699]) with padding (0, 21, 0, 31)\n",
      "Padding tensor from torch.Size([1, 2593, 2699]) with padding (0, 21, 0, 31)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 1 - Image: torch.Size([3, 2197, 2573]), Mask: torch.Size([1, 2197, 2573])\n",
      "Padding tensor from torch.Size([3, 2197, 2573]) with padding (0, 147, 0, 427)\n",
      "Padding tensor from torch.Size([1, 2197, 2573]) with padding (0, 147, 0, 427)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 2 - Image: torch.Size([3, 2454, 2649]), Mask: torch.Size([1, 2454, 2649])\n",
      "Padding tensor from torch.Size([3, 2454, 2649]) with padding (0, 71, 0, 170)\n",
      "Padding tensor from torch.Size([1, 2454, 2649]) with padding (0, 71, 0, 170)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 3 - Image: torch.Size([3, 1743, 2050]), Mask: torch.Size([1, 1743, 2050])\n",
      "Padding tensor from torch.Size([3, 1743, 2050]) with padding (0, 670, 0, 881)\n",
      "Padding tensor from torch.Size([1, 1743, 2050]) with padding (0, 670, 0, 881)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2624, 2720]), Masks: torch.Size([4, 1, 2624, 2720])\n",
      "Processing training batch 1/5\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 1 loss: 0.5164\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 2 loss: 0.5035\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 3 loss: 0.4993\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 4 loss: 0.5375\n",
      "Batch 1 average loss: 0.5142\n",
      "Item 0 after alignment: Image torch.Size([3, 2199, 2639]), Mask torch.Size([1, 2199, 2639])\n",
      "Item 1 after alignment: Image torch.Size([3, 2146, 2256]), Mask torch.Size([1, 2146, 2256])\n",
      "Item 2 after alignment: Image torch.Size([3, 1870, 2232]), Mask torch.Size([1, 1870, 2232])\n",
      "Item 3 after alignment: Image torch.Size([3, 2350, 2472]), Mask torch.Size([1, 2350, 2472])\n",
      "Max dimensions in batch: Height=2350, Width=2639\n",
      "Target dimensions: Height=2368, Width=2656\n",
      "Processing item 0 - Image: torch.Size([3, 2199, 2639]), Mask: torch.Size([1, 2199, 2639])\n",
      "Padding tensor from torch.Size([3, 2199, 2639]) with padding (0, 17, 0, 169)\n",
      "Padding tensor from torch.Size([1, 2199, 2639]) with padding (0, 17, 0, 169)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2368, 2656]), Mask: torch.Size([1, 2368, 2656])\n",
      "Processing item 1 - Image: torch.Size([3, 2146, 2256]), Mask: torch.Size([1, 2146, 2256])\n",
      "Padding tensor from torch.Size([3, 2146, 2256]) with padding (0, 400, 0, 222)\n",
      "Padding tensor from torch.Size([1, 2146, 2256]) with padding (0, 400, 0, 222)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2368, 2656]), Mask: torch.Size([1, 2368, 2656])\n",
      "Processing item 2 - Image: torch.Size([3, 1870, 2232]), Mask: torch.Size([1, 1870, 2232])\n",
      "Padding tensor from torch.Size([3, 1870, 2232]) with padding (0, 424, 0, 498)\n",
      "Padding tensor from torch.Size([1, 1870, 2232]) with padding (0, 424, 0, 498)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2368, 2656]), Mask: torch.Size([1, 2368, 2656])\n",
      "Processing item 3 - Image: torch.Size([3, 2350, 2472]), Mask: torch.Size([1, 2350, 2472])\n",
      "Padding tensor from torch.Size([3, 2350, 2472]) with padding (0, 184, 0, 18)\n",
      "Padding tensor from torch.Size([1, 2350, 2472]) with padding (0, 184, 0, 18)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2368, 2656]), Mask: torch.Size([1, 2368, 2656])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2368, 2656]), Masks: torch.Size([4, 1, 2368, 2656])\n",
      "Processing training batch 2/5\n",
      "Converted image shape: (2368, 2656, 3), mask shape: (2368, 2656, 1)\n",
      "Image 1 loss: 0.5215\n",
      "Converted image shape: (2368, 2656, 3), mask shape: (2368, 2656, 1)\n",
      "Image 2 loss: 0.5258\n",
      "Converted image shape: (2368, 2656, 3), mask shape: (2368, 2656, 1)\n",
      "Image 3 loss: 0.5248\n",
      "Converted image shape: (2368, 2656, 3), mask shape: (2368, 2656, 1)\n",
      "Image 4 loss: 0.5013\n",
      "Batch 2 average loss: 0.5183\n",
      "Item 0 after alignment: Image torch.Size([3, 2205, 2468]), Mask torch.Size([1, 2205, 2468])\n",
      "Item 1 after alignment: Image torch.Size([3, 2165, 2458]), Mask torch.Size([1, 2165, 2458])\n",
      "Item 2 after alignment: Image torch.Size([3, 2197, 2302]), Mask torch.Size([1, 2197, 2302])\n",
      "Item 3 after alignment: Image torch.Size([3, 2522, 2727]), Mask torch.Size([1, 2522, 2727])\n",
      "Max dimensions in batch: Height=2522, Width=2727\n",
      "Target dimensions: Height=2528, Width=2752\n",
      "Processing item 0 - Image: torch.Size([3, 2205, 2468]), Mask: torch.Size([1, 2205, 2468])\n",
      "Padding tensor from torch.Size([3, 2205, 2468]) with padding (0, 284, 0, 323)\n",
      "Padding tensor from torch.Size([1, 2205, 2468]) with padding (0, 284, 0, 323)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 1 - Image: torch.Size([3, 2165, 2458]), Mask: torch.Size([1, 2165, 2458])\n",
      "Padding tensor from torch.Size([3, 2165, 2458]) with padding (0, 294, 0, 363)\n",
      "Padding tensor from torch.Size([1, 2165, 2458]) with padding (0, 294, 0, 363)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 2 - Image: torch.Size([3, 2197, 2302]), Mask: torch.Size([1, 2197, 2302])\n",
      "Padding tensor from torch.Size([3, 2197, 2302]) with padding (0, 450, 0, 331)\n",
      "Padding tensor from torch.Size([1, 2197, 2302]) with padding (0, 450, 0, 331)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 3 - Image: torch.Size([3, 2522, 2727]), Mask: torch.Size([1, 2522, 2727])\n",
      "Padding tensor from torch.Size([3, 2522, 2727]) with padding (0, 25, 0, 6)\n",
      "Padding tensor from torch.Size([1, 2522, 2727]) with padding (0, 25, 0, 6)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2528, 2752]), Masks: torch.Size([4, 1, 2528, 2752])\n",
      "Processing training batch 3/5\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 1 loss: 0.4932\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 2 loss: 0.4837\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 3 loss: 0.5006\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 4 loss: 0.4838\n",
      "Batch 3 average loss: 0.4903\n",
      "Item 0 after alignment: Image torch.Size([3, 2443, 2581]), Mask torch.Size([1, 2443, 2581])\n",
      "Item 1 after alignment: Image torch.Size([3, 1837, 2049]), Mask torch.Size([1, 1837, 2049])\n",
      "Item 2 after alignment: Image torch.Size([3, 2124, 2633]), Mask torch.Size([1, 2124, 2633])\n",
      "Item 3 after alignment: Image torch.Size([3, 1679, 1972]), Mask torch.Size([1, 1679, 1972])\n",
      "Max dimensions in batch: Height=2443, Width=2633\n",
      "Target dimensions: Height=2464, Width=2656\n",
      "Processing item 0 - Image: torch.Size([3, 2443, 2581]), Mask: torch.Size([1, 2443, 2581])\n",
      "Padding tensor from torch.Size([3, 2443, 2581]) with padding (0, 75, 0, 21)\n",
      "Padding tensor from torch.Size([1, 2443, 2581]) with padding (0, 75, 0, 21)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 1 - Image: torch.Size([3, 1837, 2049]), Mask: torch.Size([1, 1837, 2049])\n",
      "Padding tensor from torch.Size([3, 1837, 2049]) with padding (0, 607, 0, 627)\n",
      "Padding tensor from torch.Size([1, 1837, 2049]) with padding (0, 607, 0, 627)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 2 - Image: torch.Size([3, 2124, 2633]), Mask: torch.Size([1, 2124, 2633])\n",
      "Padding tensor from torch.Size([3, 2124, 2633]) with padding (0, 23, 0, 340)\n",
      "Padding tensor from torch.Size([1, 2124, 2633]) with padding (0, 23, 0, 340)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 3 - Image: torch.Size([3, 1679, 1972]), Mask: torch.Size([1, 1679, 1972])\n",
      "Padding tensor from torch.Size([3, 1679, 1972]) with padding (0, 684, 0, 785)\n",
      "Padding tensor from torch.Size([1, 1679, 1972]) with padding (0, 684, 0, 785)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2464, 2656]), Masks: torch.Size([4, 1, 2464, 2656])\n",
      "Processing training batch 4/5\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 1 loss: 0.4837\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 2 loss: 0.5254\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 3 loss: 0.4762\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 4 loss: 0.4966\n",
      "Batch 4 average loss: 0.4955\n",
      "Item 0 after alignment: Image torch.Size([3, 2042, 2337]), Mask torch.Size([1, 2042, 2337])\n",
      "Item 1 after alignment: Image torch.Size([3, 1963, 2016]), Mask torch.Size([1, 1963, 2016])\n",
      "Item 2 after alignment: Image torch.Size([3, 2525, 2908]), Mask torch.Size([1, 2525, 2908])\n",
      "Max dimensions in batch: Height=2525, Width=2908\n",
      "Target dimensions: Height=2528, Width=2912\n",
      "Processing item 0 - Image: torch.Size([3, 2042, 2337]), Mask: torch.Size([1, 2042, 2337])\n",
      "Padding tensor from torch.Size([3, 2042, 2337]) with padding (0, 575, 0, 486)\n",
      "Padding tensor from torch.Size([1, 2042, 2337]) with padding (0, 575, 0, 486)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 1 - Image: torch.Size([3, 1963, 2016]), Mask: torch.Size([1, 1963, 2016])\n",
      "Padding tensor from torch.Size([3, 1963, 2016]) with padding (0, 896, 0, 565)\n",
      "Padding tensor from torch.Size([1, 1963, 2016]) with padding (0, 896, 0, 565)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 2 - Image: torch.Size([3, 2525, 2908]), Mask: torch.Size([1, 2525, 2908])\n",
      "Padding tensor from torch.Size([3, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Padding tensor from torch.Size([1, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Final batched shapes - Images: torch.Size([3, 3, 2528, 2912]), Masks: torch.Size([3, 1, 2528, 2912])\n",
      "Processing training batch 5/5\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 1 loss: 0.4899\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 2 loss: 0.4801\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 3 loss: 0.4708\n",
      "Batch 5 average loss: 0.4803\n",
      "Starting validation epoch 3\n",
      "Item 0 after alignment: Image torch.Size([3, 2067, 2332]), Mask torch.Size([1, 2067, 2332])\n",
      "Item 1 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Item 2 after alignment: Image torch.Size([3, 1972, 2400]), Mask torch.Size([1, 1972, 2400])\n",
      "Item 3 after alignment: Image torch.Size([3, 1681, 2144]), Mask torch.Size([1, 1681, 2144])\n",
      "Max dimensions in batch: Height=2067, Width=2400\n",
      "Target dimensions: Height=2080, Width=2400\n",
      "Processing item 0 - Image: torch.Size([3, 2067, 2332]), Mask: torch.Size([1, 2067, 2332])\n",
      "Padding tensor from torch.Size([3, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Padding tensor from torch.Size([1, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 1 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 2 - Image: torch.Size([3, 1972, 2400]), Mask: torch.Size([1, 1972, 2400])\n",
      "Padding tensor from torch.Size([3, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Padding tensor from torch.Size([1, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 3 - Image: torch.Size([3, 1681, 2144]), Mask: torch.Size([1, 1681, 2144])\n",
      "Padding tensor from torch.Size([3, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Padding tensor from torch.Size([1, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2080, 2400]), Masks: torch.Size([4, 1, 2080, 2400])\n",
      "Processing validation batch 1/2\n",
      "Validation image 1 loss: 1.1902\n",
      "Validation image 2 loss: 0.5763\n",
      "Validation image 3 loss: 0.5111\n",
      "Validation image 4 loss: 0.6150\n",
      "Validation batch 1 average loss: 0.7231\n",
      "Item 0 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Max dimensions in batch: Height=1705, Width=1902\n",
      "Target dimensions: Height=1728, Width=1920\n",
      "Processing item 0 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 1728, 1920]), Mask: torch.Size([1, 1728, 1920])\n",
      "Final batched shapes - Images: torch.Size([1, 3, 1728, 1920]), Masks: torch.Size([1, 1, 1728, 1920])\n",
      "Processing validation batch 2/2\n",
      "Validation image 1 loss: 0.5727\n",
      "Validation batch 2 average loss: 0.5727\n",
      "Epoch 3/10, Train Loss: 0.4997, Val Loss: 0.6479\n",
      "Starting training epoch 4\n",
      "Item 0 after alignment: Image torch.Size([3, 2197, 2573]), Mask torch.Size([1, 2197, 2573])\n",
      "Item 1 after alignment: Image torch.Size([3, 2593, 2699]), Mask torch.Size([1, 2593, 2699])\n",
      "Item 2 after alignment: Image torch.Size([3, 2165, 2458]), Mask torch.Size([1, 2165, 2458])\n",
      "Item 3 after alignment: Image torch.Size([3, 2522, 2727]), Mask torch.Size([1, 2522, 2727])\n",
      "Max dimensions in batch: Height=2593, Width=2727\n",
      "Target dimensions: Height=2624, Width=2752\n",
      "Processing item 0 - Image: torch.Size([3, 2197, 2573]), Mask: torch.Size([1, 2197, 2573])\n",
      "Padding tensor from torch.Size([3, 2197, 2573]) with padding (0, 179, 0, 427)\n",
      "Padding tensor from torch.Size([1, 2197, 2573]) with padding (0, 179, 0, 427)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2624, 2752]), Mask: torch.Size([1, 2624, 2752])\n",
      "Processing item 1 - Image: torch.Size([3, 2593, 2699]), Mask: torch.Size([1, 2593, 2699])\n",
      "Padding tensor from torch.Size([3, 2593, 2699]) with padding (0, 53, 0, 31)\n",
      "Padding tensor from torch.Size([1, 2593, 2699]) with padding (0, 53, 0, 31)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2624, 2752]), Mask: torch.Size([1, 2624, 2752])\n",
      "Processing item 2 - Image: torch.Size([3, 2165, 2458]), Mask: torch.Size([1, 2165, 2458])\n",
      "Padding tensor from torch.Size([3, 2165, 2458]) with padding (0, 294, 0, 459)\n",
      "Padding tensor from torch.Size([1, 2165, 2458]) with padding (0, 294, 0, 459)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2624, 2752]), Mask: torch.Size([1, 2624, 2752])\n",
      "Processing item 3 - Image: torch.Size([3, 2522, 2727]), Mask: torch.Size([1, 2522, 2727])\n",
      "Padding tensor from torch.Size([3, 2522, 2727]) with padding (0, 25, 0, 102)\n",
      "Padding tensor from torch.Size([1, 2522, 2727]) with padding (0, 25, 0, 102)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2624, 2752]), Mask: torch.Size([1, 2624, 2752])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2624, 2752]), Masks: torch.Size([4, 1, 2624, 2752])\n",
      "Processing training batch 1/5\n",
      "Converted image shape: (2624, 2752, 3), mask shape: (2624, 2752, 1)\n",
      "Image 1 loss: 0.4682\n",
      "Converted image shape: (2624, 2752, 3), mask shape: (2624, 2752, 1)\n",
      "Image 2 loss: 0.4663\n",
      "Converted image shape: (2624, 2752, 3), mask shape: (2624, 2752, 1)\n",
      "Image 3 loss: 0.4633\n",
      "Converted image shape: (2624, 2752, 3), mask shape: (2624, 2752, 1)\n",
      "Image 4 loss: 0.4636\n",
      "Batch 1 average loss: 0.4653\n",
      "Item 0 after alignment: Image torch.Size([3, 1743, 2050]), Mask torch.Size([1, 1743, 2050])\n",
      "Item 1 after alignment: Image torch.Size([3, 1870, 2232]), Mask torch.Size([1, 1870, 2232])\n",
      "Item 2 after alignment: Image torch.Size([3, 2454, 2649]), Mask torch.Size([1, 2454, 2649])\n",
      "Item 3 after alignment: Image torch.Size([3, 2042, 2337]), Mask torch.Size([1, 2042, 2337])\n",
      "Max dimensions in batch: Height=2454, Width=2649\n",
      "Target dimensions: Height=2464, Width=2656\n",
      "Processing item 0 - Image: torch.Size([3, 1743, 2050]), Mask: torch.Size([1, 1743, 2050])\n",
      "Padding tensor from torch.Size([3, 1743, 2050]) with padding (0, 606, 0, 721)\n",
      "Padding tensor from torch.Size([1, 1743, 2050]) with padding (0, 606, 0, 721)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 1 - Image: torch.Size([3, 1870, 2232]), Mask: torch.Size([1, 1870, 2232])\n",
      "Padding tensor from torch.Size([3, 1870, 2232]) with padding (0, 424, 0, 594)\n",
      "Padding tensor from torch.Size([1, 1870, 2232]) with padding (0, 424, 0, 594)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 2 - Image: torch.Size([3, 2454, 2649]), Mask: torch.Size([1, 2454, 2649])\n",
      "Padding tensor from torch.Size([3, 2454, 2649]) with padding (0, 7, 0, 10)\n",
      "Padding tensor from torch.Size([1, 2454, 2649]) with padding (0, 7, 0, 10)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 3 - Image: torch.Size([3, 2042, 2337]), Mask: torch.Size([1, 2042, 2337])\n",
      "Padding tensor from torch.Size([3, 2042, 2337]) with padding (0, 319, 0, 422)\n",
      "Padding tensor from torch.Size([1, 2042, 2337]) with padding (0, 319, 0, 422)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2464, 2656]), Masks: torch.Size([4, 1, 2464, 2656])\n",
      "Processing training batch 2/5\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 1 loss: 0.4681\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 2 loss: 0.4665\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 3 loss: 0.4601\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 4 loss: 0.4551\n",
      "Batch 2 average loss: 0.4625\n",
      "Item 0 after alignment: Image torch.Size([3, 2199, 2639]), Mask torch.Size([1, 2199, 2639])\n",
      "Item 1 after alignment: Image torch.Size([3, 1679, 1972]), Mask torch.Size([1, 1679, 1972])\n",
      "Item 2 after alignment: Image torch.Size([3, 1963, 2016]), Mask torch.Size([1, 1963, 2016])\n",
      "Item 3 after alignment: Image torch.Size([3, 2146, 2256]), Mask torch.Size([1, 2146, 2256])\n",
      "Max dimensions in batch: Height=2199, Width=2639\n",
      "Target dimensions: Height=2208, Width=2656\n",
      "Processing item 0 - Image: torch.Size([3, 2199, 2639]), Mask: torch.Size([1, 2199, 2639])\n",
      "Padding tensor from torch.Size([3, 2199, 2639]) with padding (0, 17, 0, 9)\n",
      "Padding tensor from torch.Size([1, 2199, 2639]) with padding (0, 17, 0, 9)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Processing item 1 - Image: torch.Size([3, 1679, 1972]), Mask: torch.Size([1, 1679, 1972])\n",
      "Padding tensor from torch.Size([3, 1679, 1972]) with padding (0, 684, 0, 529)\n",
      "Padding tensor from torch.Size([1, 1679, 1972]) with padding (0, 684, 0, 529)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Processing item 2 - Image: torch.Size([3, 1963, 2016]), Mask: torch.Size([1, 1963, 2016])\n",
      "Padding tensor from torch.Size([3, 1963, 2016]) with padding (0, 640, 0, 245)\n",
      "Padding tensor from torch.Size([1, 1963, 2016]) with padding (0, 640, 0, 245)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Processing item 3 - Image: torch.Size([3, 2146, 2256]), Mask: torch.Size([1, 2146, 2256])\n",
      "Padding tensor from torch.Size([3, 2146, 2256]) with padding (0, 400, 0, 62)\n",
      "Padding tensor from torch.Size([1, 2146, 2256]) with padding (0, 400, 0, 62)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2208, 2656]), Masks: torch.Size([4, 1, 2208, 2656])\n",
      "Processing training batch 3/5\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 1 loss: 0.4484\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 2 loss: 0.4534\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 3 loss: 0.4562\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 4 loss: 0.4447\n",
      "Batch 3 average loss: 0.4507\n",
      "Item 0 after alignment: Image torch.Size([3, 2525, 2908]), Mask torch.Size([1, 2525, 2908])\n",
      "Item 1 after alignment: Image torch.Size([3, 2443, 2581]), Mask torch.Size([1, 2443, 2581])\n",
      "Item 2 after alignment: Image torch.Size([3, 1837, 2049]), Mask torch.Size([1, 1837, 2049])\n",
      "Item 3 after alignment: Image torch.Size([3, 2124, 2633]), Mask torch.Size([1, 2124, 2633])\n",
      "Max dimensions in batch: Height=2525, Width=2908\n",
      "Target dimensions: Height=2528, Width=2912\n",
      "Processing item 0 - Image: torch.Size([3, 2525, 2908]), Mask: torch.Size([1, 2525, 2908])\n",
      "Padding tensor from torch.Size([3, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Padding tensor from torch.Size([1, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 1 - Image: torch.Size([3, 2443, 2581]), Mask: torch.Size([1, 2443, 2581])\n",
      "Padding tensor from torch.Size([3, 2443, 2581]) with padding (0, 331, 0, 85)\n",
      "Padding tensor from torch.Size([1, 2443, 2581]) with padding (0, 331, 0, 85)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 2 - Image: torch.Size([3, 1837, 2049]), Mask: torch.Size([1, 1837, 2049])\n",
      "Padding tensor from torch.Size([3, 1837, 2049]) with padding (0, 863, 0, 691)\n",
      "Padding tensor from torch.Size([1, 1837, 2049]) with padding (0, 863, 0, 691)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 3 - Image: torch.Size([3, 2124, 2633]), Mask: torch.Size([1, 2124, 2633])\n",
      "Padding tensor from torch.Size([3, 2124, 2633]) with padding (0, 279, 0, 404)\n",
      "Padding tensor from torch.Size([1, 2124, 2633]) with padding (0, 279, 0, 404)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2528, 2912]), Masks: torch.Size([4, 1, 2528, 2912])\n",
      "Processing training batch 4/5\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 1 loss: 0.4463\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 2 loss: 0.4473\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 3 loss: 0.4710\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 4 loss: 0.4483\n",
      "Batch 4 average loss: 0.4532\n",
      "Item 0 after alignment: Image torch.Size([3, 2197, 2302]), Mask torch.Size([1, 2197, 2302])\n",
      "Item 1 after alignment: Image torch.Size([3, 2350, 2472]), Mask torch.Size([1, 2350, 2472])\n",
      "Item 2 after alignment: Image torch.Size([3, 2205, 2468]), Mask torch.Size([1, 2205, 2468])\n",
      "Max dimensions in batch: Height=2350, Width=2472\n",
      "Target dimensions: Height=2368, Width=2496\n",
      "Processing item 0 - Image: torch.Size([3, 2197, 2302]), Mask: torch.Size([1, 2197, 2302])\n",
      "Padding tensor from torch.Size([3, 2197, 2302]) with padding (0, 194, 0, 171)\n",
      "Padding tensor from torch.Size([1, 2197, 2302]) with padding (0, 194, 0, 171)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2368, 2496]), Mask: torch.Size([1, 2368, 2496])\n",
      "Processing item 1 - Image: torch.Size([3, 2350, 2472]), Mask: torch.Size([1, 2350, 2472])\n",
      "Padding tensor from torch.Size([3, 2350, 2472]) with padding (0, 24, 0, 18)\n",
      "Padding tensor from torch.Size([1, 2350, 2472]) with padding (0, 24, 0, 18)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2368, 2496]), Mask: torch.Size([1, 2368, 2496])\n",
      "Processing item 2 - Image: torch.Size([3, 2205, 2468]), Mask: torch.Size([1, 2205, 2468])\n",
      "Padding tensor from torch.Size([3, 2205, 2468]) with padding (0, 28, 0, 163)\n",
      "Padding tensor from torch.Size([1, 2205, 2468]) with padding (0, 28, 0, 163)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2368, 2496]), Mask: torch.Size([1, 2368, 2496])\n",
      "Final batched shapes - Images: torch.Size([3, 3, 2368, 2496]), Masks: torch.Size([3, 1, 2368, 2496])\n",
      "Processing training batch 5/5\n",
      "Converted image shape: (2368, 2496, 3), mask shape: (2368, 2496, 1)\n",
      "Image 1 loss: 0.4374\n",
      "Converted image shape: (2368, 2496, 3), mask shape: (2368, 2496, 1)\n",
      "Image 2 loss: 0.4429\n",
      "Converted image shape: (2368, 2496, 3), mask shape: (2368, 2496, 1)\n",
      "Image 3 loss: 0.4406\n",
      "Batch 5 average loss: 0.4403\n",
      "Starting validation epoch 4\n",
      "Item 0 after alignment: Image torch.Size([3, 2067, 2332]), Mask torch.Size([1, 2067, 2332])\n",
      "Item 1 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Item 2 after alignment: Image torch.Size([3, 1972, 2400]), Mask torch.Size([1, 1972, 2400])\n",
      "Item 3 after alignment: Image torch.Size([3, 1681, 2144]), Mask torch.Size([1, 1681, 2144])\n",
      "Max dimensions in batch: Height=2067, Width=2400\n",
      "Target dimensions: Height=2080, Width=2400\n",
      "Processing item 0 - Image: torch.Size([3, 2067, 2332]), Mask: torch.Size([1, 2067, 2332])\n",
      "Padding tensor from torch.Size([3, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Padding tensor from torch.Size([1, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 1 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 2 - Image: torch.Size([3, 1972, 2400]), Mask: torch.Size([1, 1972, 2400])\n",
      "Padding tensor from torch.Size([3, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Padding tensor from torch.Size([1, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 3 - Image: torch.Size([3, 1681, 2144]), Mask: torch.Size([1, 1681, 2144])\n",
      "Padding tensor from torch.Size([3, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Padding tensor from torch.Size([1, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2080, 2400]), Masks: torch.Size([4, 1, 2080, 2400])\n",
      "Processing validation batch 1/2\n",
      "Validation image 1 loss: 1.7001\n",
      "Validation image 2 loss: 0.7531\n",
      "Validation image 3 loss: 0.7037\n",
      "Validation image 4 loss: 0.7750\n",
      "Validation batch 1 average loss: 0.9830\n",
      "Item 0 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Max dimensions in batch: Height=1705, Width=1902\n",
      "Target dimensions: Height=1728, Width=1920\n",
      "Processing item 0 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 1728, 1920]), Mask: torch.Size([1, 1728, 1920])\n",
      "Final batched shapes - Images: torch.Size([1, 3, 1728, 1920]), Masks: torch.Size([1, 1, 1728, 1920])\n",
      "Processing validation batch 2/2\n",
      "Validation image 1 loss: 0.7805\n",
      "Validation batch 2 average loss: 0.7805\n",
      "Epoch 4/10, Train Loss: 0.4544, Val Loss: 0.8817\n",
      "Starting training epoch 5\n",
      "Item 0 after alignment: Image torch.Size([3, 2454, 2649]), Mask torch.Size([1, 2454, 2649])\n",
      "Item 1 after alignment: Image torch.Size([3, 2522, 2727]), Mask torch.Size([1, 2522, 2727])\n",
      "Item 2 after alignment: Image torch.Size([3, 2042, 2337]), Mask torch.Size([1, 2042, 2337])\n",
      "Item 3 after alignment: Image torch.Size([3, 2443, 2581]), Mask torch.Size([1, 2443, 2581])\n",
      "Max dimensions in batch: Height=2522, Width=2727\n",
      "Target dimensions: Height=2528, Width=2752\n",
      "Processing item 0 - Image: torch.Size([3, 2454, 2649]), Mask: torch.Size([1, 2454, 2649])\n",
      "Padding tensor from torch.Size([3, 2454, 2649]) with padding (0, 103, 0, 74)\n",
      "Padding tensor from torch.Size([1, 2454, 2649]) with padding (0, 103, 0, 74)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 1 - Image: torch.Size([3, 2522, 2727]), Mask: torch.Size([1, 2522, 2727])\n",
      "Padding tensor from torch.Size([3, 2522, 2727]) with padding (0, 25, 0, 6)\n",
      "Padding tensor from torch.Size([1, 2522, 2727]) with padding (0, 25, 0, 6)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 2 - Image: torch.Size([3, 2042, 2337]), Mask: torch.Size([1, 2042, 2337])\n",
      "Padding tensor from torch.Size([3, 2042, 2337]) with padding (0, 415, 0, 486)\n",
      "Padding tensor from torch.Size([1, 2042, 2337]) with padding (0, 415, 0, 486)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 3 - Image: torch.Size([3, 2443, 2581]), Mask: torch.Size([1, 2443, 2581])\n",
      "Padding tensor from torch.Size([3, 2443, 2581]) with padding (0, 171, 0, 85)\n",
      "Padding tensor from torch.Size([1, 2443, 2581]) with padding (0, 171, 0, 85)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2528, 2752]), Masks: torch.Size([4, 1, 2528, 2752])\n",
      "Processing training batch 1/5\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 1 loss: 0.4452\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 2 loss: 0.4429\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 3 loss: 0.4339\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 4 loss: 0.4375\n",
      "Batch 1 average loss: 0.4399\n",
      "Item 0 after alignment: Image torch.Size([3, 2197, 2302]), Mask torch.Size([1, 2197, 2302])\n",
      "Item 1 after alignment: Image torch.Size([3, 2593, 2699]), Mask torch.Size([1, 2593, 2699])\n",
      "Item 2 after alignment: Image torch.Size([3, 2146, 2256]), Mask torch.Size([1, 2146, 2256])\n",
      "Item 3 after alignment: Image torch.Size([3, 2197, 2573]), Mask torch.Size([1, 2197, 2573])\n",
      "Max dimensions in batch: Height=2593, Width=2699\n",
      "Target dimensions: Height=2624, Width=2720\n",
      "Processing item 0 - Image: torch.Size([3, 2197, 2302]), Mask: torch.Size([1, 2197, 2302])\n",
      "Padding tensor from torch.Size([3, 2197, 2302]) with padding (0, 418, 0, 427)\n",
      "Padding tensor from torch.Size([1, 2197, 2302]) with padding (0, 418, 0, 427)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 1 - Image: torch.Size([3, 2593, 2699]), Mask: torch.Size([1, 2593, 2699])\n",
      "Padding tensor from torch.Size([3, 2593, 2699]) with padding (0, 21, 0, 31)\n",
      "Padding tensor from torch.Size([1, 2593, 2699]) with padding (0, 21, 0, 31)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 2 - Image: torch.Size([3, 2146, 2256]), Mask: torch.Size([1, 2146, 2256])\n",
      "Padding tensor from torch.Size([3, 2146, 2256]) with padding (0, 464, 0, 478)\n",
      "Padding tensor from torch.Size([1, 2146, 2256]) with padding (0, 464, 0, 478)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 3 - Image: torch.Size([3, 2197, 2573]), Mask: torch.Size([1, 2197, 2573])\n",
      "Padding tensor from torch.Size([3, 2197, 2573]) with padding (0, 147, 0, 427)\n",
      "Padding tensor from torch.Size([1, 2197, 2573]) with padding (0, 147, 0, 427)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2624, 2720]), Masks: torch.Size([4, 1, 2624, 2720])\n",
      "Processing training batch 2/5\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 1 loss: 0.4326\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 2 loss: 0.4344\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 3 loss: 0.4313\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 4 loss: 0.4342\n",
      "Batch 2 average loss: 0.4331\n",
      "Item 0 after alignment: Image torch.Size([3, 2525, 2908]), Mask torch.Size([1, 2525, 2908])\n",
      "Item 1 after alignment: Image torch.Size([3, 1679, 1972]), Mask torch.Size([1, 1679, 1972])\n",
      "Item 2 after alignment: Image torch.Size([3, 2350, 2472]), Mask torch.Size([1, 2350, 2472])\n",
      "Item 3 after alignment: Image torch.Size([3, 1963, 2016]), Mask torch.Size([1, 1963, 2016])\n",
      "Max dimensions in batch: Height=2525, Width=2908\n",
      "Target dimensions: Height=2528, Width=2912\n",
      "Processing item 0 - Image: torch.Size([3, 2525, 2908]), Mask: torch.Size([1, 2525, 2908])\n",
      "Padding tensor from torch.Size([3, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Padding tensor from torch.Size([1, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 1 - Image: torch.Size([3, 1679, 1972]), Mask: torch.Size([1, 1679, 1972])\n",
      "Padding tensor from torch.Size([3, 1679, 1972]) with padding (0, 940, 0, 849)\n",
      "Padding tensor from torch.Size([1, 1679, 1972]) with padding (0, 940, 0, 849)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 2 - Image: torch.Size([3, 2350, 2472]), Mask: torch.Size([1, 2350, 2472])\n",
      "Padding tensor from torch.Size([3, 2350, 2472]) with padding (0, 440, 0, 178)\n",
      "Padding tensor from torch.Size([1, 2350, 2472]) with padding (0, 440, 0, 178)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 3 - Image: torch.Size([3, 1963, 2016]), Mask: torch.Size([1, 1963, 2016])\n",
      "Padding tensor from torch.Size([3, 1963, 2016]) with padding (0, 896, 0, 565)\n",
      "Padding tensor from torch.Size([1, 1963, 2016]) with padding (0, 896, 0, 565)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2528, 2912]), Masks: torch.Size([4, 1, 2528, 2912])\n",
      "Processing training batch 3/5\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 1 loss: 0.4265\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 2 loss: 0.4465\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 3 loss: 0.4291\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 4 loss: 0.4349\n",
      "Batch 3 average loss: 0.4343\n",
      "Item 0 after alignment: Image torch.Size([3, 2205, 2468]), Mask torch.Size([1, 2205, 2468])\n",
      "Item 1 after alignment: Image torch.Size([3, 1870, 2232]), Mask torch.Size([1, 1870, 2232])\n",
      "Item 2 after alignment: Image torch.Size([3, 2165, 2458]), Mask torch.Size([1, 2165, 2458])\n",
      "Item 3 after alignment: Image torch.Size([3, 1743, 2050]), Mask torch.Size([1, 1743, 2050])\n",
      "Max dimensions in batch: Height=2205, Width=2468\n",
      "Target dimensions: Height=2208, Width=2496\n",
      "Processing item 0 - Image: torch.Size([3, 2205, 2468]), Mask: torch.Size([1, 2205, 2468])\n",
      "Padding tensor from torch.Size([3, 2205, 2468]) with padding (0, 28, 0, 3)\n",
      "Padding tensor from torch.Size([1, 2205, 2468]) with padding (0, 28, 0, 3)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2208, 2496]), Mask: torch.Size([1, 2208, 2496])\n",
      "Processing item 1 - Image: torch.Size([3, 1870, 2232]), Mask: torch.Size([1, 1870, 2232])\n",
      "Padding tensor from torch.Size([3, 1870, 2232]) with padding (0, 264, 0, 338)\n",
      "Padding tensor from torch.Size([1, 1870, 2232]) with padding (0, 264, 0, 338)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2208, 2496]), Mask: torch.Size([1, 2208, 2496])\n",
      "Processing item 2 - Image: torch.Size([3, 2165, 2458]), Mask: torch.Size([1, 2165, 2458])\n",
      "Padding tensor from torch.Size([3, 2165, 2458]) with padding (0, 38, 0, 43)\n",
      "Padding tensor from torch.Size([1, 2165, 2458]) with padding (0, 38, 0, 43)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2208, 2496]), Mask: torch.Size([1, 2208, 2496])\n",
      "Processing item 3 - Image: torch.Size([3, 1743, 2050]), Mask: torch.Size([1, 1743, 2050])\n",
      "Padding tensor from torch.Size([3, 1743, 2050]) with padding (0, 446, 0, 465)\n",
      "Padding tensor from torch.Size([1, 1743, 2050]) with padding (0, 446, 0, 465)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2208, 2496]), Mask: torch.Size([1, 2208, 2496])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2208, 2496]), Masks: torch.Size([4, 1, 2208, 2496])\n",
      "Processing training batch 4/5\n",
      "Converted image shape: (2208, 2496, 3), mask shape: (2208, 2496, 1)\n",
      "Image 1 loss: 0.4257\n",
      "Converted image shape: (2208, 2496, 3), mask shape: (2208, 2496, 1)\n",
      "Image 2 loss: 0.4226\n",
      "Converted image shape: (2208, 2496, 3), mask shape: (2208, 2496, 1)\n",
      "Image 3 loss: 0.4270\n",
      "Converted image shape: (2208, 2496, 3), mask shape: (2208, 2496, 1)\n",
      "Image 4 loss: 0.4223\n",
      "Batch 4 average loss: 0.4244\n",
      "Item 0 after alignment: Image torch.Size([3, 1837, 2049]), Mask torch.Size([1, 1837, 2049])\n",
      "Item 1 after alignment: Image torch.Size([3, 2199, 2639]), Mask torch.Size([1, 2199, 2639])\n",
      "Item 2 after alignment: Image torch.Size([3, 2124, 2633]), Mask torch.Size([1, 2124, 2633])\n",
      "Max dimensions in batch: Height=2199, Width=2639\n",
      "Target dimensions: Height=2208, Width=2656\n",
      "Processing item 0 - Image: torch.Size([3, 1837, 2049]), Mask: torch.Size([1, 1837, 2049])\n",
      "Padding tensor from torch.Size([3, 1837, 2049]) with padding (0, 607, 0, 371)\n",
      "Padding tensor from torch.Size([1, 1837, 2049]) with padding (0, 607, 0, 371)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Processing item 1 - Image: torch.Size([3, 2199, 2639]), Mask: torch.Size([1, 2199, 2639])\n",
      "Padding tensor from torch.Size([3, 2199, 2639]) with padding (0, 17, 0, 9)\n",
      "Padding tensor from torch.Size([1, 2199, 2639]) with padding (0, 17, 0, 9)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Processing item 2 - Image: torch.Size([3, 2124, 2633]), Mask: torch.Size([1, 2124, 2633])\n",
      "Padding tensor from torch.Size([3, 2124, 2633]) with padding (0, 23, 0, 84)\n",
      "Padding tensor from torch.Size([1, 2124, 2633]) with padding (0, 23, 0, 84)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Final batched shapes - Images: torch.Size([3, 3, 2208, 2656]), Masks: torch.Size([3, 1, 2208, 2656])\n",
      "Processing training batch 5/5\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 1 loss: 0.4286\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 2 loss: 0.4125\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 3 loss: 0.4254\n",
      "Batch 5 average loss: 0.4222\n",
      "Starting validation epoch 5\n",
      "Item 0 after alignment: Image torch.Size([3, 2067, 2332]), Mask torch.Size([1, 2067, 2332])\n",
      "Item 1 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Item 2 after alignment: Image torch.Size([3, 1972, 2400]), Mask torch.Size([1, 1972, 2400])\n",
      "Item 3 after alignment: Image torch.Size([3, 1681, 2144]), Mask torch.Size([1, 1681, 2144])\n",
      "Max dimensions in batch: Height=2067, Width=2400\n",
      "Target dimensions: Height=2080, Width=2400\n",
      "Processing item 0 - Image: torch.Size([3, 2067, 2332]), Mask: torch.Size([1, 2067, 2332])\n",
      "Padding tensor from torch.Size([3, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Padding tensor from torch.Size([1, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 1 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 2 - Image: torch.Size([3, 1972, 2400]), Mask: torch.Size([1, 1972, 2400])\n",
      "Padding tensor from torch.Size([3, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Padding tensor from torch.Size([1, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 3 - Image: torch.Size([3, 1681, 2144]), Mask: torch.Size([1, 1681, 2144])\n",
      "Padding tensor from torch.Size([3, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Padding tensor from torch.Size([1, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2080, 2400]), Masks: torch.Size([4, 1, 2080, 2400])\n",
      "Processing validation batch 1/2\n",
      "Validation image 1 loss: 12.0714\n",
      "Validation image 2 loss: 3.2924\n",
      "Validation image 3 loss: 2.6110\n",
      "Validation image 4 loss: 4.1627\n",
      "Validation batch 1 average loss: 5.5344\n",
      "Item 0 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Max dimensions in batch: Height=1705, Width=1902\n",
      "Target dimensions: Height=1728, Width=1920\n",
      "Processing item 0 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 1728, 1920]), Mask: torch.Size([1, 1728, 1920])\n",
      "Final batched shapes - Images: torch.Size([1, 3, 1728, 1920]), Masks: torch.Size([1, 1, 1728, 1920])\n",
      "Processing validation batch 2/2\n",
      "Validation image 1 loss: 4.2750\n",
      "Validation batch 2 average loss: 4.2750\n",
      "Epoch 5/10, Train Loss: 0.4308, Val Loss: 4.9047\n",
      "Starting training epoch 6\n",
      "Item 0 after alignment: Image torch.Size([3, 2443, 2581]), Mask torch.Size([1, 2443, 2581])\n",
      "Item 1 after alignment: Image torch.Size([3, 2205, 2468]), Mask torch.Size([1, 2205, 2468])\n",
      "Item 2 after alignment: Image torch.Size([3, 2124, 2633]), Mask torch.Size([1, 2124, 2633])\n",
      "Item 3 after alignment: Image torch.Size([3, 2197, 2302]), Mask torch.Size([1, 2197, 2302])\n",
      "Max dimensions in batch: Height=2443, Width=2633\n",
      "Target dimensions: Height=2464, Width=2656\n",
      "Processing item 0 - Image: torch.Size([3, 2443, 2581]), Mask: torch.Size([1, 2443, 2581])\n",
      "Padding tensor from torch.Size([3, 2443, 2581]) with padding (0, 75, 0, 21)\n",
      "Padding tensor from torch.Size([1, 2443, 2581]) with padding (0, 75, 0, 21)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 1 - Image: torch.Size([3, 2205, 2468]), Mask: torch.Size([1, 2205, 2468])\n",
      "Padding tensor from torch.Size([3, 2205, 2468]) with padding (0, 188, 0, 259)\n",
      "Padding tensor from torch.Size([1, 2205, 2468]) with padding (0, 188, 0, 259)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 2 - Image: torch.Size([3, 2124, 2633]), Mask: torch.Size([1, 2124, 2633])\n",
      "Padding tensor from torch.Size([3, 2124, 2633]) with padding (0, 23, 0, 340)\n",
      "Padding tensor from torch.Size([1, 2124, 2633]) with padding (0, 23, 0, 340)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 3 - Image: torch.Size([3, 2197, 2302]), Mask: torch.Size([1, 2197, 2302])\n",
      "Padding tensor from torch.Size([3, 2197, 2302]) with padding (0, 354, 0, 267)\n",
      "Padding tensor from torch.Size([1, 2197, 2302]) with padding (0, 354, 0, 267)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2464, 2656]), Masks: torch.Size([4, 1, 2464, 2656])\n",
      "Processing training batch 1/5\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 1 loss: 0.4198\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 2 loss: 0.4188\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 3 loss: 0.4231\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 4 loss: 0.4118\n",
      "Batch 1 average loss: 0.4184\n",
      "Item 0 after alignment: Image torch.Size([3, 1963, 2016]), Mask torch.Size([1, 1963, 2016])\n",
      "Item 1 after alignment: Image torch.Size([3, 1870, 2232]), Mask torch.Size([1, 1870, 2232])\n",
      "Item 2 after alignment: Image torch.Size([3, 1837, 2049]), Mask torch.Size([1, 1837, 2049])\n",
      "Item 3 after alignment: Image torch.Size([3, 2454, 2649]), Mask torch.Size([1, 2454, 2649])\n",
      "Max dimensions in batch: Height=2454, Width=2649\n",
      "Target dimensions: Height=2464, Width=2656\n",
      "Processing item 0 - Image: torch.Size([3, 1963, 2016]), Mask: torch.Size([1, 1963, 2016])\n",
      "Padding tensor from torch.Size([3, 1963, 2016]) with padding (0, 640, 0, 501)\n",
      "Padding tensor from torch.Size([1, 1963, 2016]) with padding (0, 640, 0, 501)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 1 - Image: torch.Size([3, 1870, 2232]), Mask: torch.Size([1, 1870, 2232])\n",
      "Padding tensor from torch.Size([3, 1870, 2232]) with padding (0, 424, 0, 594)\n",
      "Padding tensor from torch.Size([1, 1870, 2232]) with padding (0, 424, 0, 594)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 2 - Image: torch.Size([3, 1837, 2049]), Mask: torch.Size([1, 1837, 2049])\n",
      "Padding tensor from torch.Size([3, 1837, 2049]) with padding (0, 607, 0, 627)\n",
      "Padding tensor from torch.Size([1, 1837, 2049]) with padding (0, 607, 0, 627)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 3 - Image: torch.Size([3, 2454, 2649]), Mask: torch.Size([1, 2454, 2649])\n",
      "Padding tensor from torch.Size([3, 2454, 2649]) with padding (0, 7, 0, 10)\n",
      "Padding tensor from torch.Size([1, 2454, 2649]) with padding (0, 7, 0, 10)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2464, 2656]), Masks: torch.Size([4, 1, 2464, 2656])\n",
      "Processing training batch 2/5\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 1 loss: 0.4213\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 2 loss: 0.4137\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 3 loss: 0.4189\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 4 loss: 0.4221\n",
      "Batch 2 average loss: 0.4190\n",
      "Item 0 after alignment: Image torch.Size([3, 2593, 2699]), Mask torch.Size([1, 2593, 2699])\n",
      "Item 1 after alignment: Image torch.Size([3, 2525, 2908]), Mask torch.Size([1, 2525, 2908])\n",
      "Item 2 after alignment: Image torch.Size([3, 1679, 1972]), Mask torch.Size([1, 1679, 1972])\n",
      "Item 3 after alignment: Image torch.Size([3, 2197, 2573]), Mask torch.Size([1, 2197, 2573])\n",
      "Max dimensions in batch: Height=2593, Width=2908\n",
      "Target dimensions: Height=2624, Width=2912\n",
      "Processing item 0 - Image: torch.Size([3, 2593, 2699]), Mask: torch.Size([1, 2593, 2699])\n",
      "Padding tensor from torch.Size([3, 2593, 2699]) with padding (0, 213, 0, 31)\n",
      "Padding tensor from torch.Size([1, 2593, 2699]) with padding (0, 213, 0, 31)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2624, 2912]), Mask: torch.Size([1, 2624, 2912])\n",
      "Processing item 1 - Image: torch.Size([3, 2525, 2908]), Mask: torch.Size([1, 2525, 2908])\n",
      "Padding tensor from torch.Size([3, 2525, 2908]) with padding (0, 4, 0, 99)\n",
      "Padding tensor from torch.Size([1, 2525, 2908]) with padding (0, 4, 0, 99)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2624, 2912]), Mask: torch.Size([1, 2624, 2912])\n",
      "Processing item 2 - Image: torch.Size([3, 1679, 1972]), Mask: torch.Size([1, 1679, 1972])\n",
      "Padding tensor from torch.Size([3, 1679, 1972]) with padding (0, 940, 0, 945)\n",
      "Padding tensor from torch.Size([1, 1679, 1972]) with padding (0, 940, 0, 945)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2624, 2912]), Mask: torch.Size([1, 2624, 2912])\n",
      "Processing item 3 - Image: torch.Size([3, 2197, 2573]), Mask: torch.Size([1, 2197, 2573])\n",
      "Padding tensor from torch.Size([3, 2197, 2573]) with padding (0, 339, 0, 427)\n",
      "Padding tensor from torch.Size([1, 2197, 2573]) with padding (0, 339, 0, 427)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2624, 2912]), Mask: torch.Size([1, 2624, 2912])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2624, 2912]), Masks: torch.Size([4, 1, 2624, 2912])\n",
      "Processing training batch 3/5\n",
      "Converted image shape: (2624, 2912, 3), mask shape: (2624, 2912, 1)\n",
      "Image 1 loss: 0.4161\n",
      "Converted image shape: (2624, 2912, 3), mask shape: (2624, 2912, 1)\n",
      "Image 2 loss: 0.4104\n",
      "Converted image shape: (2624, 2912, 3), mask shape: (2624, 2912, 1)\n",
      "Image 3 loss: 0.4227\n",
      "Converted image shape: (2624, 2912, 3), mask shape: (2624, 2912, 1)\n",
      "Image 4 loss: 0.4155\n",
      "Batch 3 average loss: 0.4162\n",
      "Item 0 after alignment: Image torch.Size([3, 2522, 2727]), Mask torch.Size([1, 2522, 2727])\n",
      "Item 1 after alignment: Image torch.Size([3, 2146, 2256]), Mask torch.Size([1, 2146, 2256])\n",
      "Item 2 after alignment: Image torch.Size([3, 2350, 2472]), Mask torch.Size([1, 2350, 2472])\n",
      "Item 3 after alignment: Image torch.Size([3, 1743, 2050]), Mask torch.Size([1, 1743, 2050])\n",
      "Max dimensions in batch: Height=2522, Width=2727\n",
      "Target dimensions: Height=2528, Width=2752\n",
      "Processing item 0 - Image: torch.Size([3, 2522, 2727]), Mask: torch.Size([1, 2522, 2727])\n",
      "Padding tensor from torch.Size([3, 2522, 2727]) with padding (0, 25, 0, 6)\n",
      "Padding tensor from torch.Size([1, 2522, 2727]) with padding (0, 25, 0, 6)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 1 - Image: torch.Size([3, 2146, 2256]), Mask: torch.Size([1, 2146, 2256])\n",
      "Padding tensor from torch.Size([3, 2146, 2256]) with padding (0, 496, 0, 382)\n",
      "Padding tensor from torch.Size([1, 2146, 2256]) with padding (0, 496, 0, 382)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 2 - Image: torch.Size([3, 2350, 2472]), Mask: torch.Size([1, 2350, 2472])\n",
      "Padding tensor from torch.Size([3, 2350, 2472]) with padding (0, 280, 0, 178)\n",
      "Padding tensor from torch.Size([1, 2350, 2472]) with padding (0, 280, 0, 178)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 3 - Image: torch.Size([3, 1743, 2050]), Mask: torch.Size([1, 1743, 2050])\n",
      "Padding tensor from torch.Size([3, 1743, 2050]) with padding (0, 702, 0, 785)\n",
      "Padding tensor from torch.Size([1, 1743, 2050]) with padding (0, 702, 0, 785)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2528, 2752]), Masks: torch.Size([4, 1, 2528, 2752])\n",
      "Processing training batch 4/5\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 1 loss: 0.4159\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 2 loss: 0.4041\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 3 loss: 0.4105\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 4 loss: 0.4093\n",
      "Batch 4 average loss: 0.4099\n",
      "Item 0 after alignment: Image torch.Size([3, 2042, 2337]), Mask torch.Size([1, 2042, 2337])\n",
      "Item 1 after alignment: Image torch.Size([3, 2165, 2458]), Mask torch.Size([1, 2165, 2458])\n",
      "Item 2 after alignment: Image torch.Size([3, 2199, 2639]), Mask torch.Size([1, 2199, 2639])\n",
      "Max dimensions in batch: Height=2199, Width=2639\n",
      "Target dimensions: Height=2208, Width=2656\n",
      "Processing item 0 - Image: torch.Size([3, 2042, 2337]), Mask: torch.Size([1, 2042, 2337])\n",
      "Padding tensor from torch.Size([3, 2042, 2337]) with padding (0, 319, 0, 166)\n",
      "Padding tensor from torch.Size([1, 2042, 2337]) with padding (0, 319, 0, 166)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Processing item 1 - Image: torch.Size([3, 2165, 2458]), Mask: torch.Size([1, 2165, 2458])\n",
      "Padding tensor from torch.Size([3, 2165, 2458]) with padding (0, 198, 0, 43)\n",
      "Padding tensor from torch.Size([1, 2165, 2458]) with padding (0, 198, 0, 43)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Processing item 2 - Image: torch.Size([3, 2199, 2639]), Mask: torch.Size([1, 2199, 2639])\n",
      "Padding tensor from torch.Size([3, 2199, 2639]) with padding (0, 17, 0, 9)\n",
      "Padding tensor from torch.Size([1, 2199, 2639]) with padding (0, 17, 0, 9)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Final batched shapes - Images: torch.Size([3, 3, 2208, 2656]), Masks: torch.Size([3, 1, 2208, 2656])\n",
      "Processing training batch 5/5\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 1 loss: 0.3931\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 2 loss: 0.4089\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 3 loss: 0.3962\n",
      "Batch 5 average loss: 0.3994\n",
      "Starting validation epoch 6\n",
      "Item 0 after alignment: Image torch.Size([3, 2067, 2332]), Mask torch.Size([1, 2067, 2332])\n",
      "Item 1 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Item 2 after alignment: Image torch.Size([3, 1972, 2400]), Mask torch.Size([1, 1972, 2400])\n",
      "Item 3 after alignment: Image torch.Size([3, 1681, 2144]), Mask torch.Size([1, 1681, 2144])\n",
      "Max dimensions in batch: Height=2067, Width=2400\n",
      "Target dimensions: Height=2080, Width=2400\n",
      "Processing item 0 - Image: torch.Size([3, 2067, 2332]), Mask: torch.Size([1, 2067, 2332])\n",
      "Padding tensor from torch.Size([3, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Padding tensor from torch.Size([1, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 1 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 2 - Image: torch.Size([3, 1972, 2400]), Mask: torch.Size([1, 1972, 2400])\n",
      "Padding tensor from torch.Size([3, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Padding tensor from torch.Size([1, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 3 - Image: torch.Size([3, 1681, 2144]), Mask: torch.Size([1, 1681, 2144])\n",
      "Padding tensor from torch.Size([3, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Padding tensor from torch.Size([1, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2080, 2400]), Masks: torch.Size([4, 1, 2080, 2400])\n",
      "Processing validation batch 1/2\n",
      "Validation image 1 loss: 1.1668\n",
      "Validation image 2 loss: 0.4442\n",
      "Validation image 3 loss: 0.3806\n",
      "Validation image 4 loss: 0.4585\n",
      "Validation batch 1 average loss: 0.6125\n",
      "Item 0 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Max dimensions in batch: Height=1705, Width=1902\n",
      "Target dimensions: Height=1728, Width=1920\n",
      "Processing item 0 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 1728, 1920]), Mask: torch.Size([1, 1728, 1920])\n",
      "Final batched shapes - Images: torch.Size([1, 3, 1728, 1920]), Masks: torch.Size([1, 1, 1728, 1920])\n",
      "Processing validation batch 2/2\n",
      "Validation image 1 loss: 0.4836\n",
      "Validation batch 2 average loss: 0.4836\n",
      "Epoch 6/10, Train Loss: 0.4126, Val Loss: 0.5480\n",
      "Starting training epoch 7\n",
      "Item 0 after alignment: Image torch.Size([3, 2443, 2581]), Mask torch.Size([1, 2443, 2581])\n",
      "Item 1 after alignment: Image torch.Size([3, 1870, 2232]), Mask torch.Size([1, 1870, 2232])\n",
      "Item 2 after alignment: Image torch.Size([3, 2593, 2699]), Mask torch.Size([1, 2593, 2699])\n",
      "Item 3 after alignment: Image torch.Size([3, 2199, 2639]), Mask torch.Size([1, 2199, 2639])\n",
      "Max dimensions in batch: Height=2593, Width=2699\n",
      "Target dimensions: Height=2624, Width=2720\n",
      "Processing item 0 - Image: torch.Size([3, 2443, 2581]), Mask: torch.Size([1, 2443, 2581])\n",
      "Padding tensor from torch.Size([3, 2443, 2581]) with padding (0, 139, 0, 181)\n",
      "Padding tensor from torch.Size([1, 2443, 2581]) with padding (0, 139, 0, 181)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 1 - Image: torch.Size([3, 1870, 2232]), Mask: torch.Size([1, 1870, 2232])\n",
      "Padding tensor from torch.Size([3, 1870, 2232]) with padding (0, 488, 0, 754)\n",
      "Padding tensor from torch.Size([1, 1870, 2232]) with padding (0, 488, 0, 754)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 2 - Image: torch.Size([3, 2593, 2699]), Mask: torch.Size([1, 2593, 2699])\n",
      "Padding tensor from torch.Size([3, 2593, 2699]) with padding (0, 21, 0, 31)\n",
      "Padding tensor from torch.Size([1, 2593, 2699]) with padding (0, 21, 0, 31)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 3 - Image: torch.Size([3, 2199, 2639]), Mask: torch.Size([1, 2199, 2639])\n",
      "Padding tensor from torch.Size([3, 2199, 2639]) with padding (0, 81, 0, 425)\n",
      "Padding tensor from torch.Size([1, 2199, 2639]) with padding (0, 81, 0, 425)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2624, 2720]), Masks: torch.Size([4, 1, 2624, 2720])\n",
      "Processing training batch 1/5\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 1 loss: 0.4050\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 2 loss: 0.4075\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 3 loss: 0.4037\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 4 loss: 0.3956\n",
      "Batch 1 average loss: 0.4030\n",
      "Item 0 after alignment: Image torch.Size([3, 2350, 2472]), Mask torch.Size([1, 2350, 2472])\n",
      "Item 1 after alignment: Image torch.Size([3, 2042, 2337]), Mask torch.Size([1, 2042, 2337])\n",
      "Item 2 after alignment: Image torch.Size([3, 2525, 2908]), Mask torch.Size([1, 2525, 2908])\n",
      "Item 3 after alignment: Image torch.Size([3, 1837, 2049]), Mask torch.Size([1, 1837, 2049])\n",
      "Max dimensions in batch: Height=2525, Width=2908\n",
      "Target dimensions: Height=2528, Width=2912\n",
      "Processing item 0 - Image: torch.Size([3, 2350, 2472]), Mask: torch.Size([1, 2350, 2472])\n",
      "Padding tensor from torch.Size([3, 2350, 2472]) with padding (0, 440, 0, 178)\n",
      "Padding tensor from torch.Size([1, 2350, 2472]) with padding (0, 440, 0, 178)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 1 - Image: torch.Size([3, 2042, 2337]), Mask: torch.Size([1, 2042, 2337])\n",
      "Padding tensor from torch.Size([3, 2042, 2337]) with padding (0, 575, 0, 486)\n",
      "Padding tensor from torch.Size([1, 2042, 2337]) with padding (0, 575, 0, 486)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 2 - Image: torch.Size([3, 2525, 2908]), Mask: torch.Size([1, 2525, 2908])\n",
      "Padding tensor from torch.Size([3, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Padding tensor from torch.Size([1, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 3 - Image: torch.Size([3, 1837, 2049]), Mask: torch.Size([1, 1837, 2049])\n",
      "Padding tensor from torch.Size([3, 1837, 2049]) with padding (0, 863, 0, 691)\n",
      "Padding tensor from torch.Size([1, 1837, 2049]) with padding (0, 863, 0, 691)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2528, 2912]), Masks: torch.Size([4, 1, 2528, 2912])\n",
      "Processing training batch 2/5\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 1 loss: 0.3976\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 2 loss: 0.3959\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 3 loss: 0.4007\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 4 loss: 0.4099\n",
      "Batch 2 average loss: 0.4010\n",
      "Item 0 after alignment: Image torch.Size([3, 2454, 2649]), Mask torch.Size([1, 2454, 2649])\n",
      "Item 1 after alignment: Image torch.Size([3, 2165, 2458]), Mask torch.Size([1, 2165, 2458])\n",
      "Item 2 after alignment: Image torch.Size([3, 1963, 2016]), Mask torch.Size([1, 1963, 2016])\n",
      "Item 3 after alignment: Image torch.Size([3, 2197, 2302]), Mask torch.Size([1, 2197, 2302])\n",
      "Max dimensions in batch: Height=2454, Width=2649\n",
      "Target dimensions: Height=2464, Width=2656\n",
      "Processing item 0 - Image: torch.Size([3, 2454, 2649]), Mask: torch.Size([1, 2454, 2649])\n",
      "Padding tensor from torch.Size([3, 2454, 2649]) with padding (0, 7, 0, 10)\n",
      "Padding tensor from torch.Size([1, 2454, 2649]) with padding (0, 7, 0, 10)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 1 - Image: torch.Size([3, 2165, 2458]), Mask: torch.Size([1, 2165, 2458])\n",
      "Padding tensor from torch.Size([3, 2165, 2458]) with padding (0, 198, 0, 299)\n",
      "Padding tensor from torch.Size([1, 2165, 2458]) with padding (0, 198, 0, 299)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 2 - Image: torch.Size([3, 1963, 2016]), Mask: torch.Size([1, 1963, 2016])\n",
      "Padding tensor from torch.Size([3, 1963, 2016]) with padding (0, 640, 0, 501)\n",
      "Padding tensor from torch.Size([1, 1963, 2016]) with padding (0, 640, 0, 501)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 3 - Image: torch.Size([3, 2197, 2302]), Mask: torch.Size([1, 2197, 2302])\n",
      "Padding tensor from torch.Size([3, 2197, 2302]) with padding (0, 354, 0, 267)\n",
      "Padding tensor from torch.Size([1, 2197, 2302]) with padding (0, 354, 0, 267)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2464, 2656]), Masks: torch.Size([4, 1, 2464, 2656])\n",
      "Processing training batch 3/5\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 1 loss: 0.4136\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 2 loss: 0.4093\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 3 loss: 0.4098\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 4 loss: 0.4009\n",
      "Batch 3 average loss: 0.4084\n",
      "Item 0 after alignment: Image torch.Size([3, 2522, 2727]), Mask torch.Size([1, 2522, 2727])\n",
      "Item 1 after alignment: Image torch.Size([3, 2124, 2633]), Mask torch.Size([1, 2124, 2633])\n",
      "Item 2 after alignment: Image torch.Size([3, 1743, 2050]), Mask torch.Size([1, 1743, 2050])\n",
      "Item 3 after alignment: Image torch.Size([3, 2197, 2573]), Mask torch.Size([1, 2197, 2573])\n",
      "Max dimensions in batch: Height=2522, Width=2727\n",
      "Target dimensions: Height=2528, Width=2752\n",
      "Processing item 0 - Image: torch.Size([3, 2522, 2727]), Mask: torch.Size([1, 2522, 2727])\n",
      "Padding tensor from torch.Size([3, 2522, 2727]) with padding (0, 25, 0, 6)\n",
      "Padding tensor from torch.Size([1, 2522, 2727]) with padding (0, 25, 0, 6)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 1 - Image: torch.Size([3, 2124, 2633]), Mask: torch.Size([1, 2124, 2633])\n",
      "Padding tensor from torch.Size([3, 2124, 2633]) with padding (0, 119, 0, 404)\n",
      "Padding tensor from torch.Size([1, 2124, 2633]) with padding (0, 119, 0, 404)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 2 - Image: torch.Size([3, 1743, 2050]), Mask: torch.Size([1, 1743, 2050])\n",
      "Padding tensor from torch.Size([3, 1743, 2050]) with padding (0, 702, 0, 785)\n",
      "Padding tensor from torch.Size([1, 1743, 2050]) with padding (0, 702, 0, 785)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 3 - Image: torch.Size([3, 2197, 2573]), Mask: torch.Size([1, 2197, 2573])\n",
      "Padding tensor from torch.Size([3, 2197, 2573]) with padding (0, 179, 0, 331)\n",
      "Padding tensor from torch.Size([1, 2197, 2573]) with padding (0, 179, 0, 331)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2528, 2752]), Masks: torch.Size([4, 1, 2528, 2752])\n",
      "Processing training batch 4/5\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 1 loss: 0.4100\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 2 loss: 0.4071\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 3 loss: 0.4054\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 4 loss: 0.4033\n",
      "Batch 4 average loss: 0.4065\n",
      "Item 0 after alignment: Image torch.Size([3, 2205, 2468]), Mask torch.Size([1, 2205, 2468])\n",
      "Item 1 after alignment: Image torch.Size([3, 1679, 1972]), Mask torch.Size([1, 1679, 1972])\n",
      "Item 2 after alignment: Image torch.Size([3, 2146, 2256]), Mask torch.Size([1, 2146, 2256])\n",
      "Max dimensions in batch: Height=2205, Width=2468\n",
      "Target dimensions: Height=2208, Width=2496\n",
      "Processing item 0 - Image: torch.Size([3, 2205, 2468]), Mask: torch.Size([1, 2205, 2468])\n",
      "Padding tensor from torch.Size([3, 2205, 2468]) with padding (0, 28, 0, 3)\n",
      "Padding tensor from torch.Size([1, 2205, 2468]) with padding (0, 28, 0, 3)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2208, 2496]), Mask: torch.Size([1, 2208, 2496])\n",
      "Processing item 1 - Image: torch.Size([3, 1679, 1972]), Mask: torch.Size([1, 1679, 1972])\n",
      "Padding tensor from torch.Size([3, 1679, 1972]) with padding (0, 524, 0, 529)\n",
      "Padding tensor from torch.Size([1, 1679, 1972]) with padding (0, 524, 0, 529)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2208, 2496]), Mask: torch.Size([1, 2208, 2496])\n",
      "Processing item 2 - Image: torch.Size([3, 2146, 2256]), Mask: torch.Size([1, 2146, 2256])\n",
      "Padding tensor from torch.Size([3, 2146, 2256]) with padding (0, 240, 0, 62)\n",
      "Padding tensor from torch.Size([1, 2146, 2256]) with padding (0, 240, 0, 62)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2208, 2496]), Mask: torch.Size([1, 2208, 2496])\n",
      "Final batched shapes - Images: torch.Size([3, 3, 2208, 2496]), Masks: torch.Size([3, 1, 2208, 2496])\n",
      "Processing training batch 5/5\n",
      "Converted image shape: (2208, 2496, 3), mask shape: (2208, 2496, 1)\n",
      "Image 1 loss: 0.3984\n",
      "Converted image shape: (2208, 2496, 3), mask shape: (2208, 2496, 1)\n",
      "Image 2 loss: 0.3932\n",
      "Converted image shape: (2208, 2496, 3), mask shape: (2208, 2496, 1)\n",
      "Image 3 loss: 0.3850\n",
      "Batch 5 average loss: 0.3922\n",
      "Starting validation epoch 7\n",
      "Item 0 after alignment: Image torch.Size([3, 2067, 2332]), Mask torch.Size([1, 2067, 2332])\n",
      "Item 1 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Item 2 after alignment: Image torch.Size([3, 1972, 2400]), Mask torch.Size([1, 1972, 2400])\n",
      "Item 3 after alignment: Image torch.Size([3, 1681, 2144]), Mask torch.Size([1, 1681, 2144])\n",
      "Max dimensions in batch: Height=2067, Width=2400\n",
      "Target dimensions: Height=2080, Width=2400\n",
      "Processing item 0 - Image: torch.Size([3, 2067, 2332]), Mask: torch.Size([1, 2067, 2332])\n",
      "Padding tensor from torch.Size([3, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Padding tensor from torch.Size([1, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 1 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 2 - Image: torch.Size([3, 1972, 2400]), Mask: torch.Size([1, 1972, 2400])\n",
      "Padding tensor from torch.Size([3, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Padding tensor from torch.Size([1, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 3 - Image: torch.Size([3, 1681, 2144]), Mask: torch.Size([1, 1681, 2144])\n",
      "Padding tensor from torch.Size([3, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Padding tensor from torch.Size([1, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2080, 2400]), Masks: torch.Size([4, 1, 2080, 2400])\n",
      "Processing validation batch 1/2\n",
      "Validation image 1 loss: 0.3994\n",
      "Validation image 2 loss: 0.3212\n",
      "Validation image 3 loss: 0.3157\n",
      "Validation image 4 loss: 0.3224\n",
      "Validation batch 1 average loss: 0.3397\n",
      "Item 0 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Max dimensions in batch: Height=1705, Width=1902\n",
      "Target dimensions: Height=1728, Width=1920\n",
      "Processing item 0 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 1728, 1920]), Mask: torch.Size([1, 1728, 1920])\n",
      "Final batched shapes - Images: torch.Size([1, 3, 1728, 1920]), Masks: torch.Size([1, 1, 1728, 1920])\n",
      "Processing validation batch 2/2\n",
      "Validation image 1 loss: 0.3248\n",
      "Validation batch 2 average loss: 0.3248\n",
      "Epoch 7/10, Train Loss: 0.4022, Val Loss: 0.3322\n",
      "Starting training epoch 8\n",
      "Item 0 after alignment: Image torch.Size([3, 2146, 2256]), Mask torch.Size([1, 2146, 2256])\n",
      "Item 1 after alignment: Image torch.Size([3, 2205, 2468]), Mask torch.Size([1, 2205, 2468])\n",
      "Item 2 after alignment: Image torch.Size([3, 2197, 2573]), Mask torch.Size([1, 2197, 2573])\n",
      "Item 3 after alignment: Image torch.Size([3, 1679, 1972]), Mask torch.Size([1, 1679, 1972])\n",
      "Max dimensions in batch: Height=2205, Width=2573\n",
      "Target dimensions: Height=2208, Width=2592\n",
      "Processing item 0 - Image: torch.Size([3, 2146, 2256]), Mask: torch.Size([1, 2146, 2256])\n",
      "Padding tensor from torch.Size([3, 2146, 2256]) with padding (0, 336, 0, 62)\n",
      "Padding tensor from torch.Size([1, 2146, 2256]) with padding (0, 336, 0, 62)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2208, 2592]), Mask: torch.Size([1, 2208, 2592])\n",
      "Processing item 1 - Image: torch.Size([3, 2205, 2468]), Mask: torch.Size([1, 2205, 2468])\n",
      "Padding tensor from torch.Size([3, 2205, 2468]) with padding (0, 124, 0, 3)\n",
      "Padding tensor from torch.Size([1, 2205, 2468]) with padding (0, 124, 0, 3)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2208, 2592]), Mask: torch.Size([1, 2208, 2592])\n",
      "Processing item 2 - Image: torch.Size([3, 2197, 2573]), Mask: torch.Size([1, 2197, 2573])\n",
      "Padding tensor from torch.Size([3, 2197, 2573]) with padding (0, 19, 0, 11)\n",
      "Padding tensor from torch.Size([1, 2197, 2573]) with padding (0, 19, 0, 11)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2208, 2592]), Mask: torch.Size([1, 2208, 2592])\n",
      "Processing item 3 - Image: torch.Size([3, 1679, 1972]), Mask: torch.Size([1, 1679, 1972])\n",
      "Padding tensor from torch.Size([3, 1679, 1972]) with padding (0, 620, 0, 529)\n",
      "Padding tensor from torch.Size([1, 1679, 1972]) with padding (0, 620, 0, 529)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2208, 2592]), Mask: torch.Size([1, 2208, 2592])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2208, 2592]), Masks: torch.Size([4, 1, 2208, 2592])\n",
      "Processing training batch 1/5\n",
      "Converted image shape: (2208, 2592, 3), mask shape: (2208, 2592, 1)\n",
      "Image 1 loss: 0.3853\n",
      "Converted image shape: (2208, 2592, 3), mask shape: (2208, 2592, 1)\n",
      "Image 2 loss: 0.3929\n",
      "Converted image shape: (2208, 2592, 3), mask shape: (2208, 2592, 1)\n",
      "Image 3 loss: 0.3943\n",
      "Converted image shape: (2208, 2592, 3), mask shape: (2208, 2592, 1)\n",
      "Image 4 loss: 0.3885\n",
      "Batch 1 average loss: 0.3902\n",
      "Item 0 after alignment: Image torch.Size([3, 2124, 2633]), Mask torch.Size([1, 2124, 2633])\n",
      "Item 1 after alignment: Image torch.Size([3, 2197, 2302]), Mask torch.Size([1, 2197, 2302])\n",
      "Item 2 after alignment: Image torch.Size([3, 2199, 2639]), Mask torch.Size([1, 2199, 2639])\n",
      "Item 3 after alignment: Image torch.Size([3, 2042, 2337]), Mask torch.Size([1, 2042, 2337])\n",
      "Max dimensions in batch: Height=2199, Width=2639\n",
      "Target dimensions: Height=2208, Width=2656\n",
      "Processing item 0 - Image: torch.Size([3, 2124, 2633]), Mask: torch.Size([1, 2124, 2633])\n",
      "Padding tensor from torch.Size([3, 2124, 2633]) with padding (0, 23, 0, 84)\n",
      "Padding tensor from torch.Size([1, 2124, 2633]) with padding (0, 23, 0, 84)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Processing item 1 - Image: torch.Size([3, 2197, 2302]), Mask: torch.Size([1, 2197, 2302])\n",
      "Padding tensor from torch.Size([3, 2197, 2302]) with padding (0, 354, 0, 11)\n",
      "Padding tensor from torch.Size([1, 2197, 2302]) with padding (0, 354, 0, 11)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Processing item 2 - Image: torch.Size([3, 2199, 2639]), Mask: torch.Size([1, 2199, 2639])\n",
      "Padding tensor from torch.Size([3, 2199, 2639]) with padding (0, 17, 0, 9)\n",
      "Padding tensor from torch.Size([1, 2199, 2639]) with padding (0, 17, 0, 9)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Processing item 3 - Image: torch.Size([3, 2042, 2337]), Mask: torch.Size([1, 2042, 2337])\n",
      "Padding tensor from torch.Size([3, 2042, 2337]) with padding (0, 319, 0, 166)\n",
      "Padding tensor from torch.Size([1, 2042, 2337]) with padding (0, 319, 0, 166)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2208, 2656]), Masks: torch.Size([4, 1, 2208, 2656])\n",
      "Processing training batch 2/5\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 1 loss: 0.3962\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 2 loss: 0.3807\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 3 loss: 0.3806\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 4 loss: 0.3757\n",
      "Batch 2 average loss: 0.3833\n",
      "Item 0 after alignment: Image torch.Size([3, 2525, 2908]), Mask torch.Size([1, 2525, 2908])\n",
      "Item 1 after alignment: Image torch.Size([3, 2443, 2581]), Mask torch.Size([1, 2443, 2581])\n",
      "Item 2 after alignment: Image torch.Size([3, 1837, 2049]), Mask torch.Size([1, 1837, 2049])\n",
      "Item 3 after alignment: Image torch.Size([3, 2522, 2727]), Mask torch.Size([1, 2522, 2727])\n",
      "Max dimensions in batch: Height=2525, Width=2908\n",
      "Target dimensions: Height=2528, Width=2912\n",
      "Processing item 0 - Image: torch.Size([3, 2525, 2908]), Mask: torch.Size([1, 2525, 2908])\n",
      "Padding tensor from torch.Size([3, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Padding tensor from torch.Size([1, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 1 - Image: torch.Size([3, 2443, 2581]), Mask: torch.Size([1, 2443, 2581])\n",
      "Padding tensor from torch.Size([3, 2443, 2581]) with padding (0, 331, 0, 85)\n",
      "Padding tensor from torch.Size([1, 2443, 2581]) with padding (0, 331, 0, 85)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 2 - Image: torch.Size([3, 1837, 2049]), Mask: torch.Size([1, 1837, 2049])\n",
      "Padding tensor from torch.Size([3, 1837, 2049]) with padding (0, 863, 0, 691)\n",
      "Padding tensor from torch.Size([1, 1837, 2049]) with padding (0, 863, 0, 691)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 3 - Image: torch.Size([3, 2522, 2727]), Mask: torch.Size([1, 2522, 2727])\n",
      "Padding tensor from torch.Size([3, 2522, 2727]) with padding (0, 185, 0, 6)\n",
      "Padding tensor from torch.Size([1, 2522, 2727]) with padding (0, 185, 0, 6)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2528, 2912]), Masks: torch.Size([4, 1, 2528, 2912])\n",
      "Processing training batch 3/5\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 1 loss: 0.3832\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 2 loss: 0.3886\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 3 loss: 0.3951\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 4 loss: 0.3922\n",
      "Batch 3 average loss: 0.3898\n",
      "Item 0 after alignment: Image torch.Size([3, 1870, 2232]), Mask torch.Size([1, 1870, 2232])\n",
      "Item 1 after alignment: Image torch.Size([3, 2454, 2649]), Mask torch.Size([1, 2454, 2649])\n",
      "Item 2 after alignment: Image torch.Size([3, 2593, 2699]), Mask torch.Size([1, 2593, 2699])\n",
      "Item 3 after alignment: Image torch.Size([3, 2165, 2458]), Mask torch.Size([1, 2165, 2458])\n",
      "Max dimensions in batch: Height=2593, Width=2699\n",
      "Target dimensions: Height=2624, Width=2720\n",
      "Processing item 0 - Image: torch.Size([3, 1870, 2232]), Mask: torch.Size([1, 1870, 2232])\n",
      "Padding tensor from torch.Size([3, 1870, 2232]) with padding (0, 488, 0, 754)\n",
      "Padding tensor from torch.Size([1, 1870, 2232]) with padding (0, 488, 0, 754)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 1 - Image: torch.Size([3, 2454, 2649]), Mask: torch.Size([1, 2454, 2649])\n",
      "Padding tensor from torch.Size([3, 2454, 2649]) with padding (0, 71, 0, 170)\n",
      "Padding tensor from torch.Size([1, 2454, 2649]) with padding (0, 71, 0, 170)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 2 - Image: torch.Size([3, 2593, 2699]), Mask: torch.Size([1, 2593, 2699])\n",
      "Padding tensor from torch.Size([3, 2593, 2699]) with padding (0, 21, 0, 31)\n",
      "Padding tensor from torch.Size([1, 2593, 2699]) with padding (0, 21, 0, 31)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 3 - Image: torch.Size([3, 2165, 2458]), Mask: torch.Size([1, 2165, 2458])\n",
      "Padding tensor from torch.Size([3, 2165, 2458]) with padding (0, 262, 0, 459)\n",
      "Padding tensor from torch.Size([1, 2165, 2458]) with padding (0, 262, 0, 459)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2624, 2720]), Masks: torch.Size([4, 1, 2624, 2720])\n",
      "Processing training batch 4/5\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 1 loss: 0.3863\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 2 loss: 0.3929\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 3 loss: 0.3876\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 4 loss: 0.3893\n",
      "Batch 4 average loss: 0.3890\n",
      "Item 0 after alignment: Image torch.Size([3, 1963, 2016]), Mask torch.Size([1, 1963, 2016])\n",
      "Item 1 after alignment: Image torch.Size([3, 2350, 2472]), Mask torch.Size([1, 2350, 2472])\n",
      "Item 2 after alignment: Image torch.Size([3, 1743, 2050]), Mask torch.Size([1, 1743, 2050])\n",
      "Max dimensions in batch: Height=2350, Width=2472\n",
      "Target dimensions: Height=2368, Width=2496\n",
      "Processing item 0 - Image: torch.Size([3, 1963, 2016]), Mask: torch.Size([1, 1963, 2016])\n",
      "Padding tensor from torch.Size([3, 1963, 2016]) with padding (0, 480, 0, 405)\n",
      "Padding tensor from torch.Size([1, 1963, 2016]) with padding (0, 480, 0, 405)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2368, 2496]), Mask: torch.Size([1, 2368, 2496])\n",
      "Processing item 1 - Image: torch.Size([3, 2350, 2472]), Mask: torch.Size([1, 2350, 2472])\n",
      "Padding tensor from torch.Size([3, 2350, 2472]) with padding (0, 24, 0, 18)\n",
      "Padding tensor from torch.Size([1, 2350, 2472]) with padding (0, 24, 0, 18)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2368, 2496]), Mask: torch.Size([1, 2368, 2496])\n",
      "Processing item 2 - Image: torch.Size([3, 1743, 2050]), Mask: torch.Size([1, 1743, 2050])\n",
      "Padding tensor from torch.Size([3, 1743, 2050]) with padding (0, 446, 0, 625)\n",
      "Padding tensor from torch.Size([1, 1743, 2050]) with padding (0, 446, 0, 625)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2368, 2496]), Mask: torch.Size([1, 2368, 2496])\n",
      "Final batched shapes - Images: torch.Size([3, 3, 2368, 2496]), Masks: torch.Size([3, 1, 2368, 2496])\n",
      "Processing training batch 5/5\n",
      "Converted image shape: (2368, 2496, 3), mask shape: (2368, 2496, 1)\n",
      "Image 1 loss: 0.3883\n",
      "Converted image shape: (2368, 2496, 3), mask shape: (2368, 2496, 1)\n",
      "Image 2 loss: 0.3831\n",
      "Converted image shape: (2368, 2496, 3), mask shape: (2368, 2496, 1)\n",
      "Image 3 loss: 0.3831\n",
      "Batch 5 average loss: 0.3849\n",
      "Starting validation epoch 8\n",
      "Item 0 after alignment: Image torch.Size([3, 2067, 2332]), Mask torch.Size([1, 2067, 2332])\n",
      "Item 1 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Item 2 after alignment: Image torch.Size([3, 1972, 2400]), Mask torch.Size([1, 1972, 2400])\n",
      "Item 3 after alignment: Image torch.Size([3, 1681, 2144]), Mask torch.Size([1, 1681, 2144])\n",
      "Max dimensions in batch: Height=2067, Width=2400\n",
      "Target dimensions: Height=2080, Width=2400\n",
      "Processing item 0 - Image: torch.Size([3, 2067, 2332]), Mask: torch.Size([1, 2067, 2332])\n",
      "Padding tensor from torch.Size([3, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Padding tensor from torch.Size([1, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 1 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 2 - Image: torch.Size([3, 1972, 2400]), Mask: torch.Size([1, 1972, 2400])\n",
      "Padding tensor from torch.Size([3, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Padding tensor from torch.Size([1, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 3 - Image: torch.Size([3, 1681, 2144]), Mask: torch.Size([1, 1681, 2144])\n",
      "Padding tensor from torch.Size([3, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Padding tensor from torch.Size([1, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2080, 2400]), Masks: torch.Size([4, 1, 2080, 2400])\n",
      "Processing validation batch 1/2\n",
      "Validation image 1 loss: 1.9243\n",
      "Validation image 2 loss: 0.6997\n",
      "Validation image 3 loss: 0.5699\n",
      "Validation image 4 loss: 0.8020\n",
      "Validation batch 1 average loss: 0.9990\n",
      "Item 0 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Max dimensions in batch: Height=1705, Width=1902\n",
      "Target dimensions: Height=1728, Width=1920\n",
      "Processing item 0 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 1728, 1920]), Mask: torch.Size([1, 1728, 1920])\n",
      "Final batched shapes - Images: torch.Size([1, 3, 1728, 1920]), Masks: torch.Size([1, 1, 1728, 1920])\n",
      "Processing validation batch 2/2\n",
      "Validation image 1 loss: 0.7896\n",
      "Validation batch 2 average loss: 0.7896\n",
      "Epoch 8/10, Train Loss: 0.3874, Val Loss: 0.8943\n",
      "Starting training epoch 9\n",
      "Item 0 after alignment: Image torch.Size([3, 2443, 2581]), Mask torch.Size([1, 2443, 2581])\n",
      "Item 1 after alignment: Image torch.Size([3, 2124, 2633]), Mask torch.Size([1, 2124, 2633])\n",
      "Item 2 after alignment: Image torch.Size([3, 1743, 2050]), Mask torch.Size([1, 1743, 2050])\n",
      "Item 3 after alignment: Image torch.Size([3, 1870, 2232]), Mask torch.Size([1, 1870, 2232])\n",
      "Max dimensions in batch: Height=2443, Width=2633\n",
      "Target dimensions: Height=2464, Width=2656\n",
      "Processing item 0 - Image: torch.Size([3, 2443, 2581]), Mask: torch.Size([1, 2443, 2581])\n",
      "Padding tensor from torch.Size([3, 2443, 2581]) with padding (0, 75, 0, 21)\n",
      "Padding tensor from torch.Size([1, 2443, 2581]) with padding (0, 75, 0, 21)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 1 - Image: torch.Size([3, 2124, 2633]), Mask: torch.Size([1, 2124, 2633])\n",
      "Padding tensor from torch.Size([3, 2124, 2633]) with padding (0, 23, 0, 340)\n",
      "Padding tensor from torch.Size([1, 2124, 2633]) with padding (0, 23, 0, 340)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 2 - Image: torch.Size([3, 1743, 2050]), Mask: torch.Size([1, 1743, 2050])\n",
      "Padding tensor from torch.Size([3, 1743, 2050]) with padding (0, 606, 0, 721)\n",
      "Padding tensor from torch.Size([1, 1743, 2050]) with padding (0, 606, 0, 721)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 3 - Image: torch.Size([3, 1870, 2232]), Mask: torch.Size([1, 1870, 2232])\n",
      "Padding tensor from torch.Size([3, 1870, 2232]) with padding (0, 424, 0, 594)\n",
      "Padding tensor from torch.Size([1, 1870, 2232]) with padding (0, 424, 0, 594)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2464, 2656]), Masks: torch.Size([4, 1, 2464, 2656])\n",
      "Processing training batch 1/5\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 1 loss: 0.3825\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 2 loss: 0.3865\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 3 loss: 0.3804\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 4 loss: 0.3775\n",
      "Batch 1 average loss: 0.3817\n",
      "Item 0 after alignment: Image torch.Size([3, 2525, 2908]), Mask torch.Size([1, 2525, 2908])\n",
      "Item 1 after alignment: Image torch.Size([3, 2197, 2302]), Mask torch.Size([1, 2197, 2302])\n",
      "Item 2 after alignment: Image torch.Size([3, 2454, 2649]), Mask torch.Size([1, 2454, 2649])\n",
      "Item 3 after alignment: Image torch.Size([3, 1963, 2016]), Mask torch.Size([1, 1963, 2016])\n",
      "Max dimensions in batch: Height=2525, Width=2908\n",
      "Target dimensions: Height=2528, Width=2912\n",
      "Processing item 0 - Image: torch.Size([3, 2525, 2908]), Mask: torch.Size([1, 2525, 2908])\n",
      "Padding tensor from torch.Size([3, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Padding tensor from torch.Size([1, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 1 - Image: torch.Size([3, 2197, 2302]), Mask: torch.Size([1, 2197, 2302])\n",
      "Padding tensor from torch.Size([3, 2197, 2302]) with padding (0, 610, 0, 331)\n",
      "Padding tensor from torch.Size([1, 2197, 2302]) with padding (0, 610, 0, 331)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 2 - Image: torch.Size([3, 2454, 2649]), Mask: torch.Size([1, 2454, 2649])\n",
      "Padding tensor from torch.Size([3, 2454, 2649]) with padding (0, 263, 0, 74)\n",
      "Padding tensor from torch.Size([1, 2454, 2649]) with padding (0, 263, 0, 74)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 3 - Image: torch.Size([3, 1963, 2016]), Mask: torch.Size([1, 1963, 2016])\n",
      "Padding tensor from torch.Size([3, 1963, 2016]) with padding (0, 896, 0, 565)\n",
      "Padding tensor from torch.Size([1, 1963, 2016]) with padding (0, 896, 0, 565)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2528, 2912]), Masks: torch.Size([4, 1, 2528, 2912])\n",
      "Processing training batch 2/5\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 1 loss: 0.3737\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 2 loss: 0.3735\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 3 loss: 0.3846\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 4 loss: 0.3815\n",
      "Batch 2 average loss: 0.3783\n",
      "Item 0 after alignment: Image torch.Size([3, 2522, 2727]), Mask torch.Size([1, 2522, 2727])\n",
      "Item 1 after alignment: Image torch.Size([3, 2350, 2472]), Mask torch.Size([1, 2350, 2472])\n",
      "Item 2 after alignment: Image torch.Size([3, 1679, 1972]), Mask torch.Size([1, 1679, 1972])\n",
      "Item 3 after alignment: Image torch.Size([3, 2042, 2337]), Mask torch.Size([1, 2042, 2337])\n",
      "Max dimensions in batch: Height=2522, Width=2727\n",
      "Target dimensions: Height=2528, Width=2752\n",
      "Processing item 0 - Image: torch.Size([3, 2522, 2727]), Mask: torch.Size([1, 2522, 2727])\n",
      "Padding tensor from torch.Size([3, 2522, 2727]) with padding (0, 25, 0, 6)\n",
      "Padding tensor from torch.Size([1, 2522, 2727]) with padding (0, 25, 0, 6)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 1 - Image: torch.Size([3, 2350, 2472]), Mask: torch.Size([1, 2350, 2472])\n",
      "Padding tensor from torch.Size([3, 2350, 2472]) with padding (0, 280, 0, 178)\n",
      "Padding tensor from torch.Size([1, 2350, 2472]) with padding (0, 280, 0, 178)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 2 - Image: torch.Size([3, 1679, 1972]), Mask: torch.Size([1, 1679, 1972])\n",
      "Padding tensor from torch.Size([3, 1679, 1972]) with padding (0, 780, 0, 849)\n",
      "Padding tensor from torch.Size([1, 1679, 1972]) with padding (0, 780, 0, 849)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 3 - Image: torch.Size([3, 2042, 2337]), Mask: torch.Size([1, 2042, 2337])\n",
      "Padding tensor from torch.Size([3, 2042, 2337]) with padding (0, 415, 0, 486)\n",
      "Padding tensor from torch.Size([1, 2042, 2337]) with padding (0, 415, 0, 486)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2528, 2752]), Masks: torch.Size([4, 1, 2528, 2752])\n",
      "Processing training batch 3/5\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 1 loss: 0.3805\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 2 loss: 0.3728\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 3 loss: 0.3765\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 4 loss: 0.3624\n",
      "Batch 3 average loss: 0.3730\n",
      "Item 0 after alignment: Image torch.Size([3, 2593, 2699]), Mask torch.Size([1, 2593, 2699])\n",
      "Item 1 after alignment: Image torch.Size([3, 1837, 2049]), Mask torch.Size([1, 1837, 2049])\n",
      "Item 2 after alignment: Image torch.Size([3, 2146, 2256]), Mask torch.Size([1, 2146, 2256])\n",
      "Item 3 after alignment: Image torch.Size([3, 2197, 2573]), Mask torch.Size([1, 2197, 2573])\n",
      "Max dimensions in batch: Height=2593, Width=2699\n",
      "Target dimensions: Height=2624, Width=2720\n",
      "Processing item 0 - Image: torch.Size([3, 2593, 2699]), Mask: torch.Size([1, 2593, 2699])\n",
      "Padding tensor from torch.Size([3, 2593, 2699]) with padding (0, 21, 0, 31)\n",
      "Padding tensor from torch.Size([1, 2593, 2699]) with padding (0, 21, 0, 31)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 1 - Image: torch.Size([3, 1837, 2049]), Mask: torch.Size([1, 1837, 2049])\n",
      "Padding tensor from torch.Size([3, 1837, 2049]) with padding (0, 671, 0, 787)\n",
      "Padding tensor from torch.Size([1, 1837, 2049]) with padding (0, 671, 0, 787)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 2 - Image: torch.Size([3, 2146, 2256]), Mask: torch.Size([1, 2146, 2256])\n",
      "Padding tensor from torch.Size([3, 2146, 2256]) with padding (0, 464, 0, 478)\n",
      "Padding tensor from torch.Size([1, 2146, 2256]) with padding (0, 464, 0, 478)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 3 - Image: torch.Size([3, 2197, 2573]), Mask: torch.Size([1, 2197, 2573])\n",
      "Padding tensor from torch.Size([3, 2197, 2573]) with padding (0, 147, 0, 427)\n",
      "Padding tensor from torch.Size([1, 2197, 2573]) with padding (0, 147, 0, 427)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2624, 2720]), Masks: torch.Size([4, 1, 2624, 2720])\n",
      "Processing training batch 4/5\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 1 loss: 0.3735\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 2 loss: 0.3713\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 3 loss: 0.3619\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 4 loss: 0.3743\n",
      "Batch 4 average loss: 0.3703\n",
      "Item 0 after alignment: Image torch.Size([3, 2205, 2468]), Mask torch.Size([1, 2205, 2468])\n",
      "Item 1 after alignment: Image torch.Size([3, 2165, 2458]), Mask torch.Size([1, 2165, 2458])\n",
      "Item 2 after alignment: Image torch.Size([3, 2199, 2639]), Mask torch.Size([1, 2199, 2639])\n",
      "Max dimensions in batch: Height=2205, Width=2639\n",
      "Target dimensions: Height=2208, Width=2656\n",
      "Processing item 0 - Image: torch.Size([3, 2205, 2468]), Mask: torch.Size([1, 2205, 2468])\n",
      "Padding tensor from torch.Size([3, 2205, 2468]) with padding (0, 188, 0, 3)\n",
      "Padding tensor from torch.Size([1, 2205, 2468]) with padding (0, 188, 0, 3)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Processing item 1 - Image: torch.Size([3, 2165, 2458]), Mask: torch.Size([1, 2165, 2458])\n",
      "Padding tensor from torch.Size([3, 2165, 2458]) with padding (0, 198, 0, 43)\n",
      "Padding tensor from torch.Size([1, 2165, 2458]) with padding (0, 198, 0, 43)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Processing item 2 - Image: torch.Size([3, 2199, 2639]), Mask: torch.Size([1, 2199, 2639])\n",
      "Padding tensor from torch.Size([3, 2199, 2639]) with padding (0, 17, 0, 9)\n",
      "Padding tensor from torch.Size([1, 2199, 2639]) with padding (0, 17, 0, 9)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2208, 2656]), Mask: torch.Size([1, 2208, 2656])\n",
      "Final batched shapes - Images: torch.Size([3, 3, 2208, 2656]), Masks: torch.Size([3, 1, 2208, 2656])\n",
      "Processing training batch 5/5\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 1 loss: 0.3692\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 2 loss: 0.3729\n",
      "Converted image shape: (2208, 2656, 3), mask shape: (2208, 2656, 1)\n",
      "Image 3 loss: 0.3625\n",
      "Batch 5 average loss: 0.3682\n",
      "Starting validation epoch 9\n",
      "Item 0 after alignment: Image torch.Size([3, 2067, 2332]), Mask torch.Size([1, 2067, 2332])\n",
      "Item 1 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Item 2 after alignment: Image torch.Size([3, 1972, 2400]), Mask torch.Size([1, 1972, 2400])\n",
      "Item 3 after alignment: Image torch.Size([3, 1681, 2144]), Mask torch.Size([1, 1681, 2144])\n",
      "Max dimensions in batch: Height=2067, Width=2400\n",
      "Target dimensions: Height=2080, Width=2400\n",
      "Processing item 0 - Image: torch.Size([3, 2067, 2332]), Mask: torch.Size([1, 2067, 2332])\n",
      "Padding tensor from torch.Size([3, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Padding tensor from torch.Size([1, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 1 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 2 - Image: torch.Size([3, 1972, 2400]), Mask: torch.Size([1, 1972, 2400])\n",
      "Padding tensor from torch.Size([3, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Padding tensor from torch.Size([1, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 3 - Image: torch.Size([3, 1681, 2144]), Mask: torch.Size([1, 1681, 2144])\n",
      "Padding tensor from torch.Size([3, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Padding tensor from torch.Size([1, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2080, 2400]), Masks: torch.Size([4, 1, 2080, 2400])\n",
      "Processing validation batch 1/2\n",
      "Validation image 1 loss: 0.5890\n",
      "Validation image 2 loss: 0.3800\n",
      "Validation image 3 loss: 0.3618\n",
      "Validation image 4 loss: 0.4442\n",
      "Validation batch 1 average loss: 0.4438\n",
      "Item 0 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Max dimensions in batch: Height=1705, Width=1902\n",
      "Target dimensions: Height=1728, Width=1920\n",
      "Processing item 0 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 1728, 1920]), Mask: torch.Size([1, 1728, 1920])\n",
      "Final batched shapes - Images: torch.Size([1, 3, 1728, 1920]), Masks: torch.Size([1, 1, 1728, 1920])\n",
      "Processing validation batch 2/2\n",
      "Validation image 1 loss: 0.4103\n",
      "Validation batch 2 average loss: 0.4103\n",
      "Epoch 9/10, Train Loss: 0.3743, Val Loss: 0.4271\n",
      "Starting training epoch 10\n",
      "Item 0 after alignment: Image torch.Size([3, 1963, 2016]), Mask torch.Size([1, 1963, 2016])\n",
      "Item 1 after alignment: Image torch.Size([3, 2205, 2468]), Mask torch.Size([1, 2205, 2468])\n",
      "Item 2 after alignment: Image torch.Size([3, 2525, 2908]), Mask torch.Size([1, 2525, 2908])\n",
      "Item 3 after alignment: Image torch.Size([3, 2197, 2573]), Mask torch.Size([1, 2197, 2573])\n",
      "Max dimensions in batch: Height=2525, Width=2908\n",
      "Target dimensions: Height=2528, Width=2912\n",
      "Processing item 0 - Image: torch.Size([3, 1963, 2016]), Mask: torch.Size([1, 1963, 2016])\n",
      "Padding tensor from torch.Size([3, 1963, 2016]) with padding (0, 896, 0, 565)\n",
      "Padding tensor from torch.Size([1, 1963, 2016]) with padding (0, 896, 0, 565)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 1 - Image: torch.Size([3, 2205, 2468]), Mask: torch.Size([1, 2205, 2468])\n",
      "Padding tensor from torch.Size([3, 2205, 2468]) with padding (0, 444, 0, 323)\n",
      "Padding tensor from torch.Size([1, 2205, 2468]) with padding (0, 444, 0, 323)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 2 - Image: torch.Size([3, 2525, 2908]), Mask: torch.Size([1, 2525, 2908])\n",
      "Padding tensor from torch.Size([3, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Padding tensor from torch.Size([1, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 3 - Image: torch.Size([3, 2197, 2573]), Mask: torch.Size([1, 2197, 2573])\n",
      "Padding tensor from torch.Size([3, 2197, 2573]) with padding (0, 339, 0, 331)\n",
      "Padding tensor from torch.Size([1, 2197, 2573]) with padding (0, 339, 0, 331)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2528, 2912]), Masks: torch.Size([4, 1, 2528, 2912])\n",
      "Processing training batch 1/5\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 1 loss: 0.3723\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 2 loss: 0.3689\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 3 loss: 0.3629\n",
      "Converted image shape: (2528, 2912, 3), mask shape: (2528, 2912, 1)\n",
      "Image 4 loss: 0.3687\n",
      "Batch 1 average loss: 0.3682\n",
      "Item 0 after alignment: Image torch.Size([3, 2454, 2649]), Mask torch.Size([1, 2454, 2649])\n",
      "Item 1 after alignment: Image torch.Size([3, 1870, 2232]), Mask torch.Size([1, 1870, 2232])\n",
      "Item 2 after alignment: Image torch.Size([3, 2443, 2581]), Mask torch.Size([1, 2443, 2581])\n",
      "Item 3 after alignment: Image torch.Size([3, 2124, 2633]), Mask torch.Size([1, 2124, 2633])\n",
      "Max dimensions in batch: Height=2454, Width=2649\n",
      "Target dimensions: Height=2464, Width=2656\n",
      "Processing item 0 - Image: torch.Size([3, 2454, 2649]), Mask: torch.Size([1, 2454, 2649])\n",
      "Padding tensor from torch.Size([3, 2454, 2649]) with padding (0, 7, 0, 10)\n",
      "Padding tensor from torch.Size([1, 2454, 2649]) with padding (0, 7, 0, 10)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 1 - Image: torch.Size([3, 1870, 2232]), Mask: torch.Size([1, 1870, 2232])\n",
      "Padding tensor from torch.Size([3, 1870, 2232]) with padding (0, 424, 0, 594)\n",
      "Padding tensor from torch.Size([1, 1870, 2232]) with padding (0, 424, 0, 594)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 2 - Image: torch.Size([3, 2443, 2581]), Mask: torch.Size([1, 2443, 2581])\n",
      "Padding tensor from torch.Size([3, 2443, 2581]) with padding (0, 75, 0, 21)\n",
      "Padding tensor from torch.Size([1, 2443, 2581]) with padding (0, 75, 0, 21)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Processing item 3 - Image: torch.Size([3, 2124, 2633]), Mask: torch.Size([1, 2124, 2633])\n",
      "Padding tensor from torch.Size([3, 2124, 2633]) with padding (0, 23, 0, 340)\n",
      "Padding tensor from torch.Size([1, 2124, 2633]) with padding (0, 23, 0, 340)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2464, 2656]), Mask: torch.Size([1, 2464, 2656])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2464, 2656]), Masks: torch.Size([4, 1, 2464, 2656])\n",
      "Processing training batch 2/5\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 1 loss: 0.3713\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 2 loss: 0.3644\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 3 loss: 0.3636\n",
      "Converted image shape: (2464, 2656, 3), mask shape: (2464, 2656, 1)\n",
      "Image 4 loss: 0.3687\n",
      "Batch 2 average loss: 0.3670\n",
      "Item 0 after alignment: Image torch.Size([3, 1837, 2049]), Mask torch.Size([1, 1837, 2049])\n",
      "Item 1 after alignment: Image torch.Size([3, 2199, 2639]), Mask torch.Size([1, 2199, 2639])\n",
      "Item 2 after alignment: Image torch.Size([3, 2522, 2727]), Mask torch.Size([1, 2522, 2727])\n",
      "Item 3 after alignment: Image torch.Size([3, 2350, 2472]), Mask torch.Size([1, 2350, 2472])\n",
      "Max dimensions in batch: Height=2522, Width=2727\n",
      "Target dimensions: Height=2528, Width=2752\n",
      "Processing item 0 - Image: torch.Size([3, 1837, 2049]), Mask: torch.Size([1, 1837, 2049])\n",
      "Padding tensor from torch.Size([3, 1837, 2049]) with padding (0, 703, 0, 691)\n",
      "Padding tensor from torch.Size([1, 1837, 2049]) with padding (0, 703, 0, 691)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 1 - Image: torch.Size([3, 2199, 2639]), Mask: torch.Size([1, 2199, 2639])\n",
      "Padding tensor from torch.Size([3, 2199, 2639]) with padding (0, 113, 0, 329)\n",
      "Padding tensor from torch.Size([1, 2199, 2639]) with padding (0, 113, 0, 329)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 2 - Image: torch.Size([3, 2522, 2727]), Mask: torch.Size([1, 2522, 2727])\n",
      "Padding tensor from torch.Size([3, 2522, 2727]) with padding (0, 25, 0, 6)\n",
      "Padding tensor from torch.Size([1, 2522, 2727]) with padding (0, 25, 0, 6)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Processing item 3 - Image: torch.Size([3, 2350, 2472]), Mask: torch.Size([1, 2350, 2472])\n",
      "Padding tensor from torch.Size([3, 2350, 2472]) with padding (0, 280, 0, 178)\n",
      "Padding tensor from torch.Size([1, 2350, 2472]) with padding (0, 280, 0, 178)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2528, 2752]), Mask: torch.Size([1, 2528, 2752])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2528, 2752]), Masks: torch.Size([4, 1, 2528, 2752])\n",
      "Processing training batch 3/5\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 1 loss: 0.3644\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 2 loss: 0.3564\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 3 loss: 0.3660\n",
      "Converted image shape: (2528, 2752, 3), mask shape: (2528, 2752, 1)\n",
      "Image 4 loss: 0.3582\n",
      "Batch 3 average loss: 0.3613\n",
      "Item 0 after alignment: Image torch.Size([3, 2197, 2302]), Mask torch.Size([1, 2197, 2302])\n",
      "Item 1 after alignment: Image torch.Size([3, 2593, 2699]), Mask torch.Size([1, 2593, 2699])\n",
      "Item 2 after alignment: Image torch.Size([3, 2146, 2256]), Mask torch.Size([1, 2146, 2256])\n",
      "Item 3 after alignment: Image torch.Size([3, 2165, 2458]), Mask torch.Size([1, 2165, 2458])\n",
      "Max dimensions in batch: Height=2593, Width=2699\n",
      "Target dimensions: Height=2624, Width=2720\n",
      "Processing item 0 - Image: torch.Size([3, 2197, 2302]), Mask: torch.Size([1, 2197, 2302])\n",
      "Padding tensor from torch.Size([3, 2197, 2302]) with padding (0, 418, 0, 427)\n",
      "Padding tensor from torch.Size([1, 2197, 2302]) with padding (0, 418, 0, 427)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 1 - Image: torch.Size([3, 2593, 2699]), Mask: torch.Size([1, 2593, 2699])\n",
      "Padding tensor from torch.Size([3, 2593, 2699]) with padding (0, 21, 0, 31)\n",
      "Padding tensor from torch.Size([1, 2593, 2699]) with padding (0, 21, 0, 31)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 2 - Image: torch.Size([3, 2146, 2256]), Mask: torch.Size([1, 2146, 2256])\n",
      "Padding tensor from torch.Size([3, 2146, 2256]) with padding (0, 464, 0, 478)\n",
      "Padding tensor from torch.Size([1, 2146, 2256]) with padding (0, 464, 0, 478)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Processing item 3 - Image: torch.Size([3, 2165, 2458]), Mask: torch.Size([1, 2165, 2458])\n",
      "Padding tensor from torch.Size([3, 2165, 2458]) with padding (0, 262, 0, 459)\n",
      "Padding tensor from torch.Size([1, 2165, 2458]) with padding (0, 262, 0, 459)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2624, 2720]), Mask: torch.Size([1, 2624, 2720])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2624, 2720]), Masks: torch.Size([4, 1, 2624, 2720])\n",
      "Processing training batch 4/5\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 1 loss: 0.3525\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 2 loss: 0.3604\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 3 loss: 0.3483\n",
      "Converted image shape: (2624, 2720, 3), mask shape: (2624, 2720, 1)\n",
      "Image 4 loss: 0.3604\n",
      "Batch 4 average loss: 0.3554\n",
      "Item 0 after alignment: Image torch.Size([3, 1743, 2050]), Mask torch.Size([1, 1743, 2050])\n",
      "Item 1 after alignment: Image torch.Size([3, 2042, 2337]), Mask torch.Size([1, 2042, 2337])\n",
      "Item 2 after alignment: Image torch.Size([3, 1679, 1972]), Mask torch.Size([1, 1679, 1972])\n",
      "Max dimensions in batch: Height=2042, Width=2337\n",
      "Target dimensions: Height=2048, Width=2368\n",
      "Processing item 0 - Image: torch.Size([3, 1743, 2050]), Mask: torch.Size([1, 1743, 2050])\n",
      "Padding tensor from torch.Size([3, 1743, 2050]) with padding (0, 318, 0, 305)\n",
      "Padding tensor from torch.Size([1, 1743, 2050]) with padding (0, 318, 0, 305)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2048, 2368]), Mask: torch.Size([1, 2048, 2368])\n",
      "Processing item 1 - Image: torch.Size([3, 2042, 2337]), Mask: torch.Size([1, 2042, 2337])\n",
      "Padding tensor from torch.Size([3, 2042, 2337]) with padding (0, 31, 0, 6)\n",
      "Padding tensor from torch.Size([1, 2042, 2337]) with padding (0, 31, 0, 6)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2048, 2368]), Mask: torch.Size([1, 2048, 2368])\n",
      "Processing item 2 - Image: torch.Size([3, 1679, 1972]), Mask: torch.Size([1, 1679, 1972])\n",
      "Padding tensor from torch.Size([3, 1679, 1972]) with padding (0, 396, 0, 369)\n",
      "Padding tensor from torch.Size([1, 1679, 1972]) with padding (0, 396, 0, 369)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2048, 2368]), Mask: torch.Size([1, 2048, 2368])\n",
      "Final batched shapes - Images: torch.Size([3, 3, 2048, 2368]), Masks: torch.Size([3, 1, 2048, 2368])\n",
      "Processing training batch 5/5\n",
      "Converted image shape: (2048, 2368, 3), mask shape: (2048, 2368, 1)\n",
      "Image 1 loss: 0.3558\n",
      "Converted image shape: (2048, 2368, 3), mask shape: (2048, 2368, 1)\n",
      "Image 2 loss: 0.3371\n",
      "Converted image shape: (2048, 2368, 3), mask shape: (2048, 2368, 1)\n",
      "Image 3 loss: 0.3480\n",
      "Batch 5 average loss: 0.3470\n",
      "Starting validation epoch 10\n",
      "Item 0 after alignment: Image torch.Size([3, 2067, 2332]), Mask torch.Size([1, 2067, 2332])\n",
      "Item 1 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Item 2 after alignment: Image torch.Size([3, 1972, 2400]), Mask torch.Size([1, 1972, 2400])\n",
      "Item 3 after alignment: Image torch.Size([3, 1681, 2144]), Mask torch.Size([1, 1681, 2144])\n",
      "Max dimensions in batch: Height=2067, Width=2400\n",
      "Target dimensions: Height=2080, Width=2400\n",
      "Processing item 0 - Image: torch.Size([3, 2067, 2332]), Mask: torch.Size([1, 2067, 2332])\n",
      "Padding tensor from torch.Size([3, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Padding tensor from torch.Size([1, 2067, 2332]) with padding (0, 68, 0, 13)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 1 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 498, 0, 375)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 2 - Image: torch.Size([3, 1972, 2400]), Mask: torch.Size([1, 1972, 2400])\n",
      "Padding tensor from torch.Size([3, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Padding tensor from torch.Size([1, 1972, 2400]) with padding (0, 0, 0, 108)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Processing item 3 - Image: torch.Size([3, 1681, 2144]), Mask: torch.Size([1, 1681, 2144])\n",
      "Padding tensor from torch.Size([3, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Padding tensor from torch.Size([1, 1681, 2144]) with padding (0, 256, 0, 399)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2080, 2400]), Mask: torch.Size([1, 2080, 2400])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2080, 2400]), Masks: torch.Size([4, 1, 2080, 2400])\n",
      "Processing validation batch 1/2\n",
      "Validation image 1 loss: 0.9963\n",
      "Validation image 2 loss: 0.4280\n",
      "Validation image 3 loss: 0.4059\n",
      "Validation image 4 loss: 0.5083\n",
      "Validation batch 1 average loss: 0.5846\n",
      "Item 0 after alignment: Image torch.Size([3, 1705, 1902]), Mask torch.Size([1, 1705, 1902])\n",
      "Max dimensions in batch: Height=1705, Width=1902\n",
      "Target dimensions: Height=1728, Width=1920\n",
      "Processing item 0 - Image: torch.Size([3, 1705, 1902]), Mask: torch.Size([1, 1705, 1902])\n",
      "Padding tensor from torch.Size([3, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Padding tensor from torch.Size([1, 1705, 1902]) with padding (0, 18, 0, 23)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 1728, 1920]), Mask: torch.Size([1, 1728, 1920])\n",
      "Final batched shapes - Images: torch.Size([1, 3, 1728, 1920]), Masks: torch.Size([1, 1, 1728, 1920])\n",
      "Processing validation batch 2/2\n",
      "Validation image 1 loss: 0.4728\n",
      "Validation batch 2 average loss: 0.4728\n",
      "Epoch 10/10, Train Loss: 0.3598, Val Loss: 0.5287\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "slice_height = 512\n",
    "slice_width = 512\n",
    "overlap_ratio = 0.2\n",
    "print(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting training epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "        print(f\"Processing training batch {batch_idx+1}/{len(train_loader)}\")\n",
    "        batch_loss = 0\n",
    "\n",
    "        # Process each image in the batch\n",
    "        for i in range(images.shape[0]):\n",
    "            # Get single image and mask\n",
    "            image = images[i]\n",
    "            mask = masks[i]\n",
    "\n",
    "            # Convert to numpy arrays\n",
    "            image_np = image.permute(1, 2, 0).cpu().numpy()  # [H, W, C]\n",
    "            mask_np = mask.permute(1, 2, 0).cpu().numpy()    # [H, W, C]\n",
    "\n",
    "            print(f\"Converted image shape: {image_np.shape}, mask shape: {mask_np.shape}\")\n",
    "\n",
    "            # Slice the image using our custom function\n",
    "            image_slices = custom_slice_image(\n",
    "                image_np=image_np,\n",
    "                slice_height=slice_height,\n",
    "                slice_width=slice_width,\n",
    "                overlap_ratio=overlap_ratio\n",
    "            )\n",
    "\n",
    "            # Process each slice\n",
    "            slice_losses = []\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for slice_data in image_slices:\n",
    "                # Get the slice image and coordinates\n",
    "                slice_image_data = slice_data[\"image\"]\n",
    "                x_min, y_min, x_max, y_max = slice_data[\"coordinates\"]\n",
    "\n",
    "                # Extract corresponding mask slice\n",
    "                slice_mask_data = mask_np[y_min:y_max, x_min:x_max, :]\n",
    "\n",
    "                # Convert back to tensors\n",
    "                slice_image_tensor = torch.from_numpy(\n",
    "                    slice_image_data.transpose(2, 0, 1)\n",
    "                ).float().unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "                slice_mask_tensor = torch.from_numpy(\n",
    "                    slice_mask_data.transpose(2, 0, 1)\n",
    "                ).float().unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "                # Forward pass\n",
    "                slice_output = model(slice_image_tensor)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = F.binary_cross_entropy_with_logits(\n",
    "                    slice_output, \n",
    "                    slice_mask_tensor,\n",
    "                    reduction='mean'\n",
    "                )\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                slice_losses.append(loss.item())\n",
    "\n",
    "            # Update weights after processing all slices\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate average loss for this image\n",
    "            if slice_losses:\n",
    "                image_loss = sum(slice_losses) / len(slice_losses)\n",
    "                batch_loss += image_loss\n",
    "                print(f\"Image {i+1} loss: {image_loss:.4f}\")\n",
    "\n",
    "        # Average loss for the batch\n",
    "        batch_loss /= images.shape[0]\n",
    "        train_loss += batch_loss\n",
    "        print(f\"Batch {batch_idx+1} average loss: {batch_loss:.4f}\")\n",
    "\n",
    "    # Calculate epoch training loss\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    print(f\"Starting validation epoch {epoch+1}\")\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(val_loader):\n",
    "            print(f\"Processing validation batch {batch_idx+1}/{len(val_loader)}\")\n",
    "            batch_loss = 0\n",
    "\n",
    "            # Process each image in the batch\n",
    "            for i in range(images.shape[0]):\n",
    "                # Get single image and mask\n",
    "                image = images[i]\n",
    "                mask = masks[i]\n",
    "\n",
    "                # Convert to numpy arrays\n",
    "                image_np = image.permute(1, 2, 0).cpu().numpy()  # [H, W, C]\n",
    "                mask_np = mask.permute(1, 2, 0).cpu().numpy()    # [H, W, C]\n",
    "\n",
    "                # Slice the image using our custom function\n",
    "                image_slices = custom_slice_image(\n",
    "                    image_np=image_np,\n",
    "                    slice_height=slice_height,\n",
    "                    slice_width=slice_width,\n",
    "                    overlap_ratio=overlap_ratio\n",
    "                )\n",
    "\n",
    "                # Process each slice\n",
    "                slice_losses = []\n",
    "\n",
    "                for slice_data in image_slices:\n",
    "                    # Get the slice image and coordinates\n",
    "                    slice_image_data = slice_data[\"image\"]\n",
    "                    x_min, y_min, x_max, y_max = slice_data[\"coordinates\"]\n",
    "\n",
    "                    # Extract corresponding mask slice\n",
    "                    slice_mask_data = mask_np[y_min:y_max, x_min:x_max, :]\n",
    "\n",
    "                    # Convert back to tensors\n",
    "                    slice_image_tensor = torch.from_numpy(\n",
    "                        slice_image_data.transpose(2, 0, 1)\n",
    "                    ).float().unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "                    slice_mask_tensor = torch.from_numpy(\n",
    "                        slice_mask_data.transpose(2, 0, 1)\n",
    "                    ).float().unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "                    # Forward pass\n",
    "                    slice_output = model(slice_image_tensor)\n",
    "\n",
    "                    # Calculate loss\n",
    "                    loss = F.binary_cross_entropy_with_logits(\n",
    "                        slice_output, \n",
    "                        slice_mask_tensor,\n",
    "                        reduction='mean'\n",
    "                    )\n",
    "\n",
    "                    slice_losses.append(loss.item())\n",
    "\n",
    "                # Calculate average loss for this image\n",
    "                if slice_losses:\n",
    "                    image_loss = sum(slice_losses) / len(slice_losses)\n",
    "                    batch_loss += image_loss\n",
    "                    print(f\"Validation image {i+1} loss: {image_loss:.4f}\")\n",
    "\n",
    "            # Average loss for the batch\n",
    "            batch_loss /= images.shape[0]\n",
    "            val_loss += batch_loss\n",
    "            print(f\"Validation batch {batch_idx+1} average loss: {batch_loss:.4f}\")\n",
    "\n",
    "        # Calculate epoch validation loss\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "    }, f'model_checkpoint_epoch_{epoch+1}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 1\n",
      "Item 0 after alignment: Image torch.Size([3, 1870, 2232]), Mask torch.Size([1, 1870, 2232])\n",
      "Item 1 after alignment: Image torch.Size([3, 2197, 2573]), Mask torch.Size([1, 2197, 2573])\n",
      "Item 2 after alignment: Image torch.Size([3, 2042, 2337]), Mask torch.Size([1, 2042, 2337])\n",
      "Item 3 after alignment: Image torch.Size([3, 1743, 2050]), Mask torch.Size([1, 1743, 2050])\n",
      "Max dimensions in batch: Height=2197, Width=2573\n",
      "Target dimensions: Height=2208, Width=2592\n",
      "Processing item 0 - Image: torch.Size([3, 1870, 2232]), Mask: torch.Size([1, 1870, 2232])\n",
      "Padding tensor from torch.Size([3, 1870, 2232]) with padding (0, 360, 0, 338)\n",
      "Padding tensor from torch.Size([1, 1870, 2232]) with padding (0, 360, 0, 338)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2208, 2592]), Mask: torch.Size([1, 2208, 2592])\n",
      "Processing item 1 - Image: torch.Size([3, 2197, 2573]), Mask: torch.Size([1, 2197, 2573])\n",
      "Padding tensor from torch.Size([3, 2197, 2573]) with padding (0, 19, 0, 11)\n",
      "Padding tensor from torch.Size([1, 2197, 2573]) with padding (0, 19, 0, 11)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2208, 2592]), Mask: torch.Size([1, 2208, 2592])\n",
      "Processing item 2 - Image: torch.Size([3, 2042, 2337]), Mask: torch.Size([1, 2042, 2337])\n",
      "Padding tensor from torch.Size([3, 2042, 2337]) with padding (0, 255, 0, 166)\n",
      "Padding tensor from torch.Size([1, 2042, 2337]) with padding (0, 255, 0, 166)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2208, 2592]), Mask: torch.Size([1, 2208, 2592])\n",
      "Processing item 3 - Image: torch.Size([3, 1743, 2050]), Mask: torch.Size([1, 1743, 2050])\n",
      "Padding tensor from torch.Size([3, 1743, 2050]) with padding (0, 542, 0, 465)\n",
      "Padding tensor from torch.Size([1, 1743, 2050]) with padding (0, 542, 0, 465)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2208, 2592]), Mask: torch.Size([1, 2208, 2592])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2208, 2592]), Masks: torch.Size([4, 1, 2208, 2592])\n",
      "Processing training batch 1/5\n",
      "Image 1 loss: 0.3504\n",
      "Image 2 loss: 0.3538\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[152]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# Option 2: Using weight parameter (alternative)\u001b[39;00m\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# loss = F.binary_cross_entropy_with_logits(\u001b[39;00m\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m#     slice_output, \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m \n\u001b[32m     79\u001b[39m     \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m     80\u001b[39m     masked_loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     slice_losses.append(\u001b[43mmasked_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Update weights after processing all slices\u001b[39;00m\n\u001b[32m     84\u001b[39m optimizer.step()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "slice_height = 512\n",
    "slice_width = 512\n",
    "overlap_ratio = 0.2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting training epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "        print(f\"Processing training batch {batch_idx+1}/{len(train_loader)}\")\n",
    "        batch_loss = 0\n",
    "\n",
    "        # Process each image in the batch\n",
    "        for i in range(images.shape[0]):\n",
    "            # Get single image and mask\n",
    "            image = images[i]\n",
    "            mask = masks[i]\n",
    "\n",
    "            # Convert to numpy arrays\n",
    "            image_np = image.permute(1, 2, 0).cpu().numpy()  # [H, W, C]\n",
    "            mask_np = mask.permute(1, 2, 0).cpu().numpy()    # [H, W, C]\n",
    "\n",
    "            # Slice the image using our custom function\n",
    "            image_slices = custom_slice_image(\n",
    "                image_np=image_np,\n",
    "                slice_height=slice_height,\n",
    "                slice_width=slice_width,\n",
    "                overlap_ratio=overlap_ratio\n",
    "            )\n",
    "\n",
    "            # Process each slice\n",
    "            slice_losses = []\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for slice_data in image_slices:\n",
    "                # Get the slice image and coordinates\n",
    "                slice_image_data = slice_data[\"image\"]\n",
    "                x_min, y_min, x_max, y_max = slice_data[\"coordinates\"]\n",
    "\n",
    "                # Extract corresponding mask slice\n",
    "                slice_mask_data = mask_np[y_min:y_max, x_min:x_max, :]\n",
    "\n",
    "                # Convert back to tensors\n",
    "                slice_image_tensor = torch.from_numpy(\n",
    "                    slice_image_data.transpose(2, 0, 1)\n",
    "                ).float().unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "                slice_mask_tensor = torch.from_numpy(\n",
    "                    slice_mask_data.transpose(2, 0, 1)\n",
    "                ).float().unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "                # Create a mask for valid (non-padded) regions\n",
    "                # For slices, all pixels are valid since we're taking exact slices\n",
    "                valid_mask = torch.ones_like(slice_mask_tensor)\n",
    "\n",
    "                # Forward pass\n",
    "                slice_output = model(slice_image_tensor)\n",
    "\n",
    "                # Calculate loss with masking\n",
    "                # Option 1: Using reduction='none' and manual masking\n",
    "                loss = F.binary_cross_entropy_with_logits(\n",
    "                    slice_output, \n",
    "                    slice_mask_tensor,\n",
    "                    reduction='none'\n",
    "                )\n",
    "                # Apply mask and calculate mean over valid pixels\n",
    "                masked_loss = (loss * valid_mask).sum() / valid_mask.sum()\n",
    "\n",
    "                # Option 2: Using weight parameter (alternative)\n",
    "                # loss = F.binary_cross_entropy_with_logits(\n",
    "                #     slice_output, \n",
    "                #     slice_mask_tensor,\n",
    "                #     weight=valid_mask,  # Weight parameter acts as our mask\n",
    "                #     reduction='mean'\n",
    "                # )\n",
    "\n",
    "                # Backward pass\n",
    "                masked_loss.backward()\n",
    "                slice_losses.append(masked_loss.item())\n",
    "\n",
    "            # Update weights after processing all slices\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate average loss for this image\n",
    "            if slice_losses:\n",
    "                image_loss = sum(slice_losses) / len(slice_losses)\n",
    "                batch_loss += image_loss\n",
    "                print(f\"Image {i+1} loss: {image_loss:.4f}\")\n",
    "\n",
    "        # Average loss for the batch\n",
    "        batch_loss /= images.shape[0]\n",
    "        train_loss += batch_loss\n",
    "        print(f\"Batch {batch_idx+1} average loss: {batch_loss:.4f}\")\n",
    "\n",
    "    # Calculate epoch training loss\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    print(f\"Starting validation epoch {epoch+1}\")\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(val_loader):\n",
    "            print(f\"Processing validation batch {batch_idx+1}/{len(val_loader)}\")\n",
    "            batch_loss = 0\n",
    "\n",
    "            # Process each image in the batch\n",
    "            for i in range(images.shape[0]):\n",
    "                # Get single image and mask\n",
    "                image = images[i]\n",
    "                mask = masks[i]\n",
    "\n",
    "                # Convert to numpy arrays\n",
    "                image_np = image.permute(1, 2, 0).cpu().numpy()  # [H, W, C]\n",
    "                mask_np = mask.permute(1, 2, 0).cpu().numpy()    # [H, W, C]\n",
    "\n",
    "                # Slice the image using our custom function\n",
    "                image_slices = custom_slice_image(\n",
    "                    image_np=image_np,\n",
    "                    slice_height=slice_height,\n",
    "                    slice_width=slice_width,\n",
    "                    overlap_ratio=overlap_ratio\n",
    "                )\n",
    "\n",
    "                # Process each slice\n",
    "                slice_losses = []\n",
    "\n",
    "                for slice_data in image_slices:\n",
    "                    # Get the slice image and coordinates\n",
    "                    slice_image_data = slice_data[\"image\"]\n",
    "                    x_min, y_min, x_max, y_max = slice_data[\"coordinates\"]\n",
    "\n",
    "                    # Extract corresponding mask slice\n",
    "                    slice_mask_data = mask_np[y_min:y_max, x_min:x_max, :]\n",
    "\n",
    "                    # Convert back to tensors\n",
    "                    slice_image_tensor = torch.from_numpy(\n",
    "                        slice_image_data.transpose(2, 0, 1)\n",
    "                    ).float().unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "                    slice_mask_tensor = torch.from_numpy(\n",
    "                        slice_mask_data.transpose(2, 0, 1)\n",
    "                    ).float().unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "                    # Create a mask for valid (non-padded) regions\n",
    "                    valid_mask = torch.ones_like(slice_mask_tensor)\n",
    "\n",
    "                    # Forward pass\n",
    "                    slice_output = model(slice_image_tensor)\n",
    "\n",
    "                    # Calculate loss with masking\n",
    "                    loss = F.binary_cross_entropy_with_logits(\n",
    "                        slice_output, \n",
    "                        slice_mask_tensor,\n",
    "                        reduction='none'\n",
    "                    )\n",
    "                    # Apply mask and calculate mean over valid pixels\n",
    "                    masked_loss = (loss * valid_mask).sum() / valid_mask.sum()\n",
    "\n",
    "                    slice_losses.append(masked_loss.item())\n",
    "\n",
    "                # Calculate average loss for this image\n",
    "                if slice_losses:\n",
    "                    image_loss = sum(slice_losses) / len(slice_losses)\n",
    "                    batch_loss += image_loss\n",
    "                    print(f\"Validation image {i+1} loss: {image_loss:.4f}\")\n",
    "\n",
    "            # Average loss for the batch\n",
    "            batch_loss /= images.shape[0]\n",
    "            val_loss += batch_loss\n",
    "            print(f\"Validation batch {batch_idx+1} average loss: {batch_loss:.4f}\")\n",
    "\n",
    "        # Calculate epoch validation loss\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "    }, f'model_checkpoint_epoch_{epoch+1}.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to mask the loss reduction and only focus on unpadded, original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 1\n",
      "Item 0 after alignment: Image torch.Size([3, 2525, 2908]), Mask torch.Size([1, 2525, 2908])\n",
      "Item 1 after alignment: Image torch.Size([3, 2197, 2302]), Mask torch.Size([1, 2197, 2302])\n",
      "Item 2 after alignment: Image torch.Size([3, 2350, 2472]), Mask torch.Size([1, 2350, 2472])\n",
      "Item 3 after alignment: Image torch.Size([3, 1679, 1972]), Mask torch.Size([1, 1679, 1972])\n",
      "Max dimensions in batch: Height=2525, Width=2908\n",
      "Target dimensions: Height=2528, Width=2912\n",
      "Processing item 0 - Image: torch.Size([3, 2525, 2908]), Mask: torch.Size([1, 2525, 2908])\n",
      "Padding tensor from torch.Size([3, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Padding tensor from torch.Size([1, 2525, 2908]) with padding (0, 4, 0, 3)\n",
      "Successfully processed item 0 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 1 - Image: torch.Size([3, 2197, 2302]), Mask: torch.Size([1, 2197, 2302])\n",
      "Padding tensor from torch.Size([3, 2197, 2302]) with padding (0, 610, 0, 331)\n",
      "Padding tensor from torch.Size([1, 2197, 2302]) with padding (0, 610, 0, 331)\n",
      "Successfully processed item 1 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 2 - Image: torch.Size([3, 2350, 2472]), Mask: torch.Size([1, 2350, 2472])\n",
      "Padding tensor from torch.Size([3, 2350, 2472]) with padding (0, 440, 0, 178)\n",
      "Padding tensor from torch.Size([1, 2350, 2472]) with padding (0, 440, 0, 178)\n",
      "Successfully processed item 2 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Processing item 3 - Image: torch.Size([3, 1679, 1972]), Mask: torch.Size([1, 1679, 1972])\n",
      "Padding tensor from torch.Size([3, 1679, 1972]) with padding (0, 940, 0, 849)\n",
      "Padding tensor from torch.Size([1, 1679, 1972]) with padding (0, 940, 0, 849)\n",
      "Successfully processed item 3 - Image: torch.Size([3, 2528, 2912]), Mask: torch.Size([1, 2528, 2912])\n",
      "Final batched shapes - Images: torch.Size([4, 3, 2528, 2912]), Masks: torch.Size([4, 1, 2528, 2912])\n",
      "Processing training batch 1/5\n",
      "Image 1 loss: 0.3514 (valid pixels: 14680064.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[151]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     97\u001b[39m         loss = masked_loss.sum() / num_valid_pixels\n\u001b[32m     98\u001b[39m         loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m         slice_losses.append(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# Update weights after processing all slices\u001b[39;00m\n\u001b[32m    102\u001b[39m optimizer.step()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "slice_height = 512\n",
    "slice_width = 512\n",
    "overlap_ratio = 0.2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting training epoch {epoch+1}\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_pixels = 0\n",
    "    valid_pixels = 0\n",
    "\n",
    "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "        print(f\"Processing training batch {batch_idx+1}/{len(train_loader)}\")\n",
    "        batch_loss = 0\n",
    "        batch_valid_pixels = 0\n",
    "\n",
    "        # Process each image in the batch\n",
    "        for i in range(images.shape[0]):\n",
    "            # Get single image and mask\n",
    "            image = images[i]\n",
    "            mask = masks[i]\n",
    "\n",
    "            # Convert to numpy arrays\n",
    "            image_np = image.permute(1, 2, 0).cpu().numpy()  # [H, W, C]\n",
    "            mask_np = mask.permute(1, 2, 0).cpu().numpy()    # [H, W, C]\n",
    "\n",
    "            # Slice the image using our custom function\n",
    "            image_slices = custom_slice_image(\n",
    "                image_np=image_np,\n",
    "                slice_height=slice_height,\n",
    "                slice_width=slice_width,\n",
    "                overlap_ratio=overlap_ratio\n",
    "            )\n",
    "\n",
    "            # Process each slice\n",
    "            slice_losses = []\n",
    "            slice_valid_pixels = 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for slice_data in image_slices:\n",
    "                # Get the slice image and coordinates\n",
    "                slice_image_data = slice_data[\"image\"]\n",
    "                x_min, y_min, x_max, y_max = slice_data[\"coordinates\"]\n",
    "\n",
    "                # Extract corresponding mask slice\n",
    "                slice_mask_data = mask_np[y_min:y_max, x_min:x_max, :]\n",
    "\n",
    "                # Convert back to tensors\n",
    "                slice_image_tensor = torch.from_numpy(\n",
    "                    slice_image_data.transpose(2, 0, 1)\n",
    "                ).float().unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "                slice_mask_tensor = torch.from_numpy(\n",
    "                    slice_mask_data.transpose(2, 0, 1)\n",
    "                ).float().unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "                # Create a validity mask for the original content (non-padded areas)\n",
    "                # For slices that come from the interior of the image, all pixels are valid\n",
    "                # For edge slices, we need to check if they were padded\n",
    "\n",
    "                # By default, all pixels in the slice are valid\n",
    "                validity_mask = torch.ones_like(slice_mask_tensor)\n",
    "\n",
    "                # If this is an edge slice (right or bottom edge of the original image),\n",
    "                # mark padded areas as invalid (0)\n",
    "                if x_max == image_np.shape[1] and x_max - x_min < slice_width:\n",
    "                    # This slice includes padding on the right\n",
    "                    valid_width = image_np.shape[1] - x_min\n",
    "                    validity_mask[:, :, :, valid_width:] = 0\n",
    "\n",
    "                if y_max == image_np.shape[0] and y_max - y_min < slice_height:\n",
    "                    # This slice includes padding on the bottom\n",
    "                    valid_height = image_np.shape[0] - y_min\n",
    "                    validity_mask[:, :, valid_height:, :] = 0\n",
    "\n",
    "                # Forward pass\n",
    "                slice_output = model(slice_image_tensor)\n",
    "\n",
    "                # Calculate masked loss\n",
    "                # Use reduction='none' to get per-pixel losses\n",
    "                per_pixel_loss = F.binary_cross_entropy_with_logits(\n",
    "                    slice_output, \n",
    "                    slice_mask_tensor,\n",
    "                    reduction='none'\n",
    "                )\n",
    "\n",
    "                # Apply validity mask to the loss\n",
    "                masked_loss = per_pixel_loss * validity_mask\n",
    "\n",
    "                # Count valid pixels for normalization\n",
    "                num_valid_pixels = validity_mask.sum().item()\n",
    "                slice_valid_pixels += num_valid_pixels\n",
    "\n",
    "                # Normalize by the number of valid pixels\n",
    "                if num_valid_pixels > 0:\n",
    "                    loss = masked_loss.sum() / num_valid_pixels\n",
    "                    loss.backward()\n",
    "                    slice_losses.append(loss.item())\n",
    "\n",
    "            # Update weights after processing all slices\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate average loss for this image\n",
    "            if slice_losses and slice_valid_pixels > 0:\n",
    "                image_loss = sum(slice_losses) / len(slice_losses)\n",
    "                batch_loss += image_loss * slice_valid_pixels\n",
    "                batch_valid_pixels += slice_valid_pixels\n",
    "                print(f\"Image {i+1} loss: {image_loss:.4f} (valid pixels: {slice_valid_pixels})\")\n",
    "\n",
    "        # Average loss for the batch, weighted by valid pixels\n",
    "        if batch_valid_pixels > 0:\n",
    "            batch_loss /= batch_valid_pixels\n",
    "            train_loss += batch_loss * batch_valid_pixels\n",
    "            total_pixels += batch_valid_pixels\n",
    "            valid_pixels += batch_valid_pixels\n",
    "            print(f\"Batch {batch_idx+1} average loss: {batch_loss:.4f} (valid pixels: {batch_valid_pixels})\")\n",
    "\n",
    "    # Calculate epoch training loss, weighted by valid pixels\n",
    "    if total_pixels > 0:\n",
    "        train_loss /= total_pixels\n",
    "\n",
    "    # Validation\n",
    "    print(f\"Starting validation epoch {epoch+1}\")\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_total_pixels = 0\n",
    "    val_valid_pixels = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(val_loader):\n",
    "            print(f\"Processing validation batch {batch_idx+1}/{len(val_loader)}\")\n",
    "            batch_loss = 0\n",
    "            batch_valid_pixels = 0\n",
    "\n",
    "            # Process each image in the batch\n",
    "            for i in range(images.shape[0]):\n",
    "                # Get single image and mask\n",
    "                image = images[i]\n",
    "                mask = masks[i]\n",
    "\n",
    "                # Convert to numpy arrays\n",
    "                image_np = image.permute(1, 2, 0).cpu().numpy()  # [H, W, C]\n",
    "                mask_np = mask.permute(1, 2, 0).cpu().numpy()    # [H, W, C]\n",
    "\n",
    "                # Slice the image using our custom function\n",
    "                image_slices = custom_slice_image(\n",
    "                    image_np=image_np,\n",
    "                    slice_height=slice_height,\n",
    "                    slice_width=slice_width,\n",
    "                    overlap_ratio=overlap_ratio\n",
    "                )\n",
    "\n",
    "                # Process each slice\n",
    "                slice_losses = []\n",
    "                slice_valid_pixels = 0\n",
    "\n",
    "                for slice_data in image_slices:\n",
    "                    # Get the slice image and coordinates\n",
    "                    slice_image_data = slice_data[\"image\"]\n",
    "                    x_min, y_min, x_max, y_max = slice_data[\"coordinates\"]\n",
    "\n",
    "                    # Extract corresponding mask slice\n",
    "                    slice_mask_data = mask_np[y_min:y_max, x_min:x_max, :]\n",
    "\n",
    "                    # Convert back to tensors\n",
    "                    slice_image_tensor = torch.from_numpy(\n",
    "                        slice_image_data.transpose(2, 0, 1)\n",
    "                    ).float().unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "                    slice_mask_tensor = torch.from_numpy(\n",
    "                        slice_mask_data.transpose(2, 0, 1)\n",
    "                    ).float().unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "                    # Create validity mask for the original content\n",
    "                    validity_mask = torch.ones_like(slice_mask_tensor)\n",
    "\n",
    "                    # Mark padded areas as invalid\n",
    "                    if x_max == image_np.shape[1] and x_max - x_min < slice_width:\n",
    "                        valid_width = image_np.shape[1] - x_min\n",
    "                        validity_mask[:, :, :, valid_width:] = 0\n",
    "\n",
    "                    if y_max == image_np.shape[0] and y_max - y_min < slice_height:\n",
    "                        valid_height = image_np.shape[0] - y_min\n",
    "                        validity_mask[:, :, valid_height:, :] = 0\n",
    "\n",
    "                    # Forward pass\n",
    "                    slice_output = model(slice_image_tensor)\n",
    "\n",
    "                    # Calculate masked loss\n",
    "                    per_pixel_loss = F.binary_cross_entropy_with_logits(\n",
    "                        slice_output, \n",
    "                        slice_mask_tensor,\n",
    "                        reduction='none'\n",
    "                    )\n",
    "\n",
    "                    # Apply validity mask\n",
    "                    masked_loss = per_pixel_loss * validity_mask\n",
    "\n",
    "                    # Count valid pixels\n",
    "                    num_valid_pixels = validity_mask.sum().item()\n",
    "                    slice_valid_pixels += num_valid_pixels\n",
    "\n",
    "                    # Normalize by valid pixels\n",
    "                    if num_valid_pixels > 0:\n",
    "                        loss = masked_loss.sum() / num_valid_pixels\n",
    "                        slice_losses.append(loss.item())\n",
    "\n",
    "                # Calculate average loss for this image\n",
    "                if slice_losses and slice_valid_pixels > 0:\n",
    "                    image_loss = sum(slice_losses) / len(slice_losses)\n",
    "                    batch_loss += image_loss * slice_valid_pixels\n",
    "                    batch_valid_pixels += slice_valid_pixels\n",
    "\n",
    "            # Average loss for the batch, weighted by valid pixels\n",
    "            if batch_valid_pixels > 0:\n",
    "                batch_loss /= batch_valid_pixels\n",
    "                val_loss += batch_loss * batch_valid_pixels\n",
    "                val_total_pixels += batch_valid_pixels\n",
    "                val_valid_pixels += batch_valid_pixels\n",
    "                print(f\"Validation batch {batch_idx+1} average loss: {batch_loss:.4f} (valid pixels: {batch_valid_pixels})\")\n",
    "\n",
    "        # Calculate epoch validation loss, weighted by valid pixels\n",
    "        if val_total_pixels > 0:\n",
    "            val_loss /= val_total_pixels\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f} (valid: {valid_pixels}/{total_pixels}), Val Loss: {val_loss:.4f} (valid: {val_valid_pixels}/{val_total_pixels})\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "    }, f'model_checkpoint_epoch_{epoch+1}.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built to only train on originals while padding degrades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"unet_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_nightly-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
